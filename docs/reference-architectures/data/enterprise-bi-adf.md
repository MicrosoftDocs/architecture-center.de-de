---
title: Automatisierte Enterprise BI-Instanz (Business Intelligence)
titleSuffix: Azure Reference Architectures
description: Automatisieren Sie einen ELT-Workflow (Extrahieren, Laden und Transformieren) in Azure mithilfe von Azure Data Factory und SQL Data Warehouse.
author: MikeWasson
ms.date: 11/06/2018
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: seodec18
ms.openlocfilehash: 020c401e9db85b76fd48c6df9be9c80d2ba5c7e4
ms.sourcegitcommit: 1b50810208354577b00e89e5c031b774b02736e2
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 01/23/2019
ms.locfileid: "54481472"
---
# <a name="automated-enterprise-bi-with-sql-data-warehouse-and-azure-data-factory"></a><span data-ttu-id="330d7-103">Automatisierte Enterprise BI-Instanz mit SQL Data Warehouse und Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="330d7-103">Automated enterprise BI with SQL Data Warehouse and Azure Data Factory</span></span>

<span data-ttu-id="330d7-104">Diese Referenzarchitektur veranschaulicht die Vorgehensweise für inkrementelles Laden in einer [ELT-Pipeline](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt) (Extrahieren, Laden und Transformieren).</span><span class="sxs-lookup"><span data-stu-id="330d7-104">This reference architecture shows how to perform incremental loading in an [extract, load, and transform (ELT)](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt) pipeline.</span></span> <span data-ttu-id="330d7-105">Sie verwendet Azure Data Factory, um die ELT-Pipeline zu automatisieren.</span><span class="sxs-lookup"><span data-stu-id="330d7-105">It uses Azure Data Factory to automate the ELT pipeline.</span></span> <span data-ttu-id="330d7-106">Die Pipeline verschiebt die neuesten OLTP-Daten inkrementell aus einer lokalen SQL Server-Datenbank in SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="330d7-106">The pipeline incrementally moves the latest OLTP data from an on-premises SQL Server database into SQL Data Warehouse.</span></span> <span data-ttu-id="330d7-107">Transaktionsdaten werden in ein tabellarisches Modell für die Analyse transformiert.</span><span class="sxs-lookup"><span data-stu-id="330d7-107">Transactional data is transformed into a tabular model for analysis.</span></span>

> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE2Gnz2]

<span data-ttu-id="330d7-108">Eine Referenzimplementierung für diese Architektur ist auf [GitHub][github] verfügbar.</span><span class="sxs-lookup"><span data-stu-id="330d7-108">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Architekturdiagramm für eine automatisierte Enterprise BI-Instanz mit SQL Data Warehouse und Azure Data Factory](./images/enterprise-bi-sqldw-adf.png)

<span data-ttu-id="330d7-110">Diese Architektur baut auf der in [Enterprise BI mit SQL Data Warehouse](./enterprise-bi-sqldw.md) gezeigten auf, weist jedoch einige zusätzliche Funktionen auf, die für Data Warehousing-Szenarien für Unternehmen wichtig sind.</span><span class="sxs-lookup"><span data-stu-id="330d7-110">This architecture builds on the one shown in [Enterprise BI with SQL Data Warehouse](./enterprise-bi-sqldw.md), but adds some features that are important for enterprise data warehousing scenarios.</span></span>

- <span data-ttu-id="330d7-111">Automatisierung der Pipeline mithilfe von Data Factory.</span><span class="sxs-lookup"><span data-stu-id="330d7-111">Automation of the pipeline using Data Factory.</span></span>
- <span data-ttu-id="330d7-112">Inkrementelles Laden.</span><span class="sxs-lookup"><span data-stu-id="330d7-112">Incremental loading.</span></span>
- <span data-ttu-id="330d7-113">Integration mehrerer Datenquellen.</span><span class="sxs-lookup"><span data-stu-id="330d7-113">Integrating multiple data sources.</span></span>
- <span data-ttu-id="330d7-114">Laden von binären Daten wie räumliche Daten und Bilder.</span><span class="sxs-lookup"><span data-stu-id="330d7-114">Loading binary data such as geospatial data and images.</span></span>

## <a name="architecture"></a><span data-ttu-id="330d7-115">Architecture</span><span class="sxs-lookup"><span data-stu-id="330d7-115">Architecture</span></span>

<span data-ttu-id="330d7-116">Die Architektur umfasst die folgenden Komponenten.</span><span class="sxs-lookup"><span data-stu-id="330d7-116">The architecture consists of the following components.</span></span>

### <a name="data-sources"></a><span data-ttu-id="330d7-117">Datenquellen</span><span class="sxs-lookup"><span data-stu-id="330d7-117">Data sources</span></span>

<span data-ttu-id="330d7-118">**Lokaler SQL Server**.</span><span class="sxs-lookup"><span data-stu-id="330d7-118">**On-premises SQL Server**.</span></span> <span data-ttu-id="330d7-119">Die Quelldaten befinden sich in einer lokalen SQL Server-Datenbank.</span><span class="sxs-lookup"><span data-stu-id="330d7-119">The source data is located in a SQL Server database on premises.</span></span> <span data-ttu-id="330d7-120">Um die lokale Umgebung zu simulieren, stellen die Bereitstellungsskripts für diese Architektur eine VM in Azure bereit, auf der SQL Server installiert ist.</span><span class="sxs-lookup"><span data-stu-id="330d7-120">To simulate the on-premises environment, the deployment scripts for this architecture provision a virtual machine in Azure with SQL Server installed.</span></span> <span data-ttu-id="330d7-121">Die [OLTP-Beispieldatenbank von Wide World Importers][wwi] wird als Quelldatenbank verwendet.</span><span class="sxs-lookup"><span data-stu-id="330d7-121">The [Wide World Importers OLTP sample database][wwi] is used as the source database.</span></span>

<span data-ttu-id="330d7-122">**Externe Daten**.</span><span class="sxs-lookup"><span data-stu-id="330d7-122">**External data**.</span></span> <span data-ttu-id="330d7-123">Ein gängiges Szenario für Data Warehouses ist die Integration mehrerer Datenquellen.</span><span class="sxs-lookup"><span data-stu-id="330d7-123">A common scenario for data warehouses is to integrate multiple data sources.</span></span> <span data-ttu-id="330d7-124">Diese Referenzarchitektur lädt ein externes Dataset, das die Stadtbevölkerung nach Jahr enthält, und integriert es in die Daten aus der OLTP-Datenbank.</span><span class="sxs-lookup"><span data-stu-id="330d7-124">This reference architecture loads an external data set that contains city populations by year, and integrates it with the data from the OLTP database.</span></span> <span data-ttu-id="330d7-125">Auf der Grundlage dieser Daten können Sie verschiedene Fragen beantworten. Beispiel: „Entspricht die Umsatzsteigerung in jeder Region dem Bevölkerungswachstum, oder ist sie unverhältnismäßig höher?“</span><span class="sxs-lookup"><span data-stu-id="330d7-125">You can use this data for insights such as: "Does sales growth in each region match or exceed population growth?"</span></span>

### <a name="ingestion-and-data-storage"></a><span data-ttu-id="330d7-126">Erfassung und Datenspeicherung</span><span class="sxs-lookup"><span data-stu-id="330d7-126">Ingestion and data storage</span></span>

<span data-ttu-id="330d7-127">**Blobspeicher**.</span><span class="sxs-lookup"><span data-stu-id="330d7-127">**Blob Storage**.</span></span> <span data-ttu-id="330d7-128">Blobspeicher wird als Stagingbereich für die Quelldaten vor dem Laden in SQL Data Warehouse verwendet.</span><span class="sxs-lookup"><span data-stu-id="330d7-128">Blob storage is used as a staging area for the source data before loading it into SQL Data Warehouse.</span></span>

<span data-ttu-id="330d7-129">**Azure SQL Data Warehouse**.</span><span class="sxs-lookup"><span data-stu-id="330d7-129">**Azure SQL Data Warehouse**.</span></span> <span data-ttu-id="330d7-130">[SQL Data Warehouse](/azure/sql-data-warehouse/) ist ein verteiltes System für die Analyse großer Datenmengen.</span><span class="sxs-lookup"><span data-stu-id="330d7-130">[SQL Data Warehouse](/azure/sql-data-warehouse/) is a distributed system designed to perform analytics on large data.</span></span> <span data-ttu-id="330d7-131">Es unterstützt massive Parallelverarbeitung (Massive Parallel Processing, MPP), die die Ausführung von Hochleistungsanalysen ermöglicht.</span><span class="sxs-lookup"><span data-stu-id="330d7-131">It supports massive parallel processing (MPP), which makes it suitable for running high-performance analytics.</span></span>

<span data-ttu-id="330d7-132">**Azure Data Factory**</span><span class="sxs-lookup"><span data-stu-id="330d7-132">**Azure Data Factory**.</span></span> <span data-ttu-id="330d7-133">[Data Factory][adf] ist ein verwalteter Dienst, der Datenverschiebung und Datentransformation orchestriert und automatisiert.</span><span class="sxs-lookup"><span data-stu-id="330d7-133">[Data Factory][adf] is a managed service that orchestrates and automates data movement and data transformation.</span></span> <span data-ttu-id="330d7-134">In dieser Architektur koordiniert er die verschiedenen Phasen des ELT-Prozesses.</span><span class="sxs-lookup"><span data-stu-id="330d7-134">In this architecture, it coordinates the various stages of the ELT process.</span></span>

### <a name="analysis-and-reporting"></a><span data-ttu-id="330d7-135">Analysen und Berichte</span><span class="sxs-lookup"><span data-stu-id="330d7-135">Analysis and reporting</span></span>

<span data-ttu-id="330d7-136">**Azure Analysis Services**:</span><span class="sxs-lookup"><span data-stu-id="330d7-136">**Azure Analysis Services**.</span></span> <span data-ttu-id="330d7-137">[Analysis Services](/azure/analysis-services/) ist ein vollständig verwalteter Dienst, der Datenmodellierungsfunktionen ermöglicht.</span><span class="sxs-lookup"><span data-stu-id="330d7-137">[Analysis Services](/azure/analysis-services/) is a fully managed service that provides data modeling capabilities.</span></span> <span data-ttu-id="330d7-138">Das semantische Modell wird in Analysis Services geladen.</span><span class="sxs-lookup"><span data-stu-id="330d7-138">The semantic model is loaded into Analysis Services.</span></span>

<span data-ttu-id="330d7-139">**Power BI**:</span><span class="sxs-lookup"><span data-stu-id="330d7-139">**Power BI**.</span></span> <span data-ttu-id="330d7-140">Power BI ist eine Suite aus Business Analytics-Tools zum Analysieren von Daten für Einblicke in Geschäftsvorgänge.</span><span class="sxs-lookup"><span data-stu-id="330d7-140">Power BI is a suite of business analytics tools to analyze data for business insights.</span></span> <span data-ttu-id="330d7-141">In dieser Architektur dient sie zum Abfragen des in Analysis Services gespeicherten semantischen Modells.</span><span class="sxs-lookup"><span data-stu-id="330d7-141">In this architecture, it queries the semantic model stored in Analysis Services.</span></span>

### <a name="authentication"></a><span data-ttu-id="330d7-142">Authentifizierung</span><span class="sxs-lookup"><span data-stu-id="330d7-142">Authentication</span></span>

<span data-ttu-id="330d7-143">**Azure Active Directory** (Azure AD) authentifiziert Benutzer, die über Power BI eine Verbindung mit dem Analysis Services-Server herstellen.</span><span class="sxs-lookup"><span data-stu-id="330d7-143">**Azure Active Directory** (Azure AD) authenticates users who connect to the Analysis Services server through Power BI.</span></span>

<span data-ttu-id="330d7-144">Data Factory auch Azure AD für die Authentifizierung bei SQL Data Warehouse nutzen, indem ein Dienstprinzipal oder eine verwaltete Dienstidentität (MSI) verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="330d7-144">Data Factory can use also use Azure AD to authenticate to SQL Data Warehouse, by using a service principal or Managed Service Identity (MSI).</span></span> <span data-ttu-id="330d7-145">Der Einfachheit halber verwendet die Beispielbereitstellung die SQL Server-Authentifizierung.</span><span class="sxs-lookup"><span data-stu-id="330d7-145">For simplicity, the example deployment uses SQL Server authentication.</span></span>

## <a name="data-pipeline"></a><span data-ttu-id="330d7-146">Datenpipeline</span><span class="sxs-lookup"><span data-stu-id="330d7-146">Data pipeline</span></span>

<span data-ttu-id="330d7-147">In [Azure Data Factory][adf] ist eine Pipeline eine logische Gruppierung von Aktivitäten zum Koordinieren einer &mdash;-Aufgabe, in diesem Fall das Laden und Transformieren von Daten in SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="330d7-147">In [Azure Data Factory][adf], a pipeline is a logical grouping of activities used to coordinate a task &mdash; in this case, loading and transforming data into SQL Data Warehouse.</span></span>

<span data-ttu-id="330d7-148">Diese Referenzarchitektur definiert eine Masterpipeline, die eine Sequenz von untergeordneten Pipelines ausführt.</span><span class="sxs-lookup"><span data-stu-id="330d7-148">This reference architecture defines a master pipeline that runs a sequence of child pipelines.</span></span> <span data-ttu-id="330d7-149">Jede untergeordnete Pipeline lädt Daten in eine oder mehrere Data Warehouse-Tabellen.</span><span class="sxs-lookup"><span data-stu-id="330d7-149">Each child pipeline loads data into one or more data warehouse tables.</span></span>

![Screenshot der Pipeline in Azure Data Factory](./images/adf-pipeline.png)

## <a name="incremental-loading"></a><span data-ttu-id="330d7-151">Inkrementelles Laden</span><span class="sxs-lookup"><span data-stu-id="330d7-151">Incremental loading</span></span>

<span data-ttu-id="330d7-152">Beim Ausführen eines automatisierten ETL-oder ELT-Prozesses ist es am effizientesten, nur die Daten zu laden, die sich seit der vorhergehenden Ausführung geändert haben.</span><span class="sxs-lookup"><span data-stu-id="330d7-152">When you run an automated ETL or ELT process, it's most efficient to load only the data that changed since the previous run.</span></span> <span data-ttu-id="330d7-153">Dies wird als *inkrementeller Ladevorgang* bezeichnet, im Gegensatz zu einem vollständige Ladevorgang, bei dem alle Daten geladen werden.</span><span class="sxs-lookup"><span data-stu-id="330d7-153">This is called an *incremental load*, as opposed to a full load that loads all of the data.</span></span> <span data-ttu-id="330d7-154">Zum Ausführen eines inkrementellen Ladevorgangs benötigen Sie eine Möglichkeit zum Identifizieren, welche Daten sich geändert haben.</span><span class="sxs-lookup"><span data-stu-id="330d7-154">To perform an incremental load, you need a way to identify which data has changed.</span></span> <span data-ttu-id="330d7-155">Der gängigste Ansatz ist die Verwendung eines Werts zum *Erkennen von Änderungen mit oberem Grenzwert*, d.h. der aktuelle Wert einer Spalte in der Quelltabelle – entweder einer datetime-Spalte oder einer eindeutigen Integer-Spalte – wird nachverfolgt.</span><span class="sxs-lookup"><span data-stu-id="330d7-155">The most common approach is to use a *high water mark* value, which means tracking the latest value of some column in the source table, either a datetime column or a unique integer column.</span></span>

<span data-ttu-id="330d7-156">Ab SQL Server 2016 können Sie [temporale Tabellen](/sql/relational-databases/tables/temporal-tables) verwenden.</span><span class="sxs-lookup"><span data-stu-id="330d7-156">Starting with SQL Server 2016, you can use [temporal tables](/sql/relational-databases/tables/temporal-tables).</span></span> <span data-ttu-id="330d7-157">Hierbei handelt es sich um Tabellen mit Systemversionsverwaltung, die einen vollständigen Verlauf aller Datenänderungen beibehalten.</span><span class="sxs-lookup"><span data-stu-id="330d7-157">These are system-versioned tables that keep a full history of data changes.</span></span> <span data-ttu-id="330d7-158">Die Datenbank-Engine zeichnet den Verlauf aller Änderungen automatisch in einer separaten Verlaufstabelle auf.</span><span class="sxs-lookup"><span data-stu-id="330d7-158">The database engine automatically records the history of every change in a separate history table.</span></span> <span data-ttu-id="330d7-159">Sie können die Verlaufsdaten abfragen, indem Sie eine FOR SYSTEM_TIME-Klausel an eine Abfrage anhängen.</span><span class="sxs-lookup"><span data-stu-id="330d7-159">You can query the historical data by adding a FOR SYSTEM_TIME clause to a query.</span></span> <span data-ttu-id="330d7-160">Intern fragt die Datenbank-Engine die Verlaufstabelle ab, dies ist für die Anwendung jedoch transparent.</span><span class="sxs-lookup"><span data-stu-id="330d7-160">Internally, the database engine queries the history table, but this is transparent to the application.</span></span>

> [!NOTE]
> <span data-ttu-id="330d7-161">Für frühere Versionen von SQL Server können Sie [Change Data Capture](/sql/relational-databases/track-changes/about-change-data-capture-sql-server) (CDC) verwenden.</span><span class="sxs-lookup"><span data-stu-id="330d7-161">For earlier versions of SQL Server, you can use [Change Data Capture](/sql/relational-databases/track-changes/about-change-data-capture-sql-server) (CDC).</span></span> <span data-ttu-id="330d7-162">Dieser Ansatz ist weniger geeignet als temporale Tabellen, da Sie eine separate Änderungstabelle abfragen müssen, und Änderungen werden anhand einer Protokollfolgenummer anstatt eines Zeitstempels nachverfolgt.</span><span class="sxs-lookup"><span data-stu-id="330d7-162">This approach is less convenient than temporal tables, because you have to query a separate change table, and changes are tracked by a log sequence number, rather than a timestamp.</span></span>
>

<span data-ttu-id="330d7-163">Temporale Tabellen sind hilfreich für Dimensionsdaten, die sich im Laufe der Zeit ändern können.</span><span class="sxs-lookup"><span data-stu-id="330d7-163">Temporal tables are useful for dimension data, which can change over time.</span></span> <span data-ttu-id="330d7-164">Faktentabellen stellen in der Regel eine unveränderliche Transaktion wie einen Verkauf dar, und in diesem Fall ist das Beibehalten des Systemversionsverlaufs nicht sinnvoll.</span><span class="sxs-lookup"><span data-stu-id="330d7-164">Fact tables usually represent an immutable transaction such as a sale, in which case keeping the system version history doesn't make sense.</span></span> <span data-ttu-id="330d7-165">Stattdessen weisen Transaktionen normalerweise eine Spalte auf, die das Transaktionsdatum darstellt, das als Wasserzeichenwert verwendet werden kann.</span><span class="sxs-lookup"><span data-stu-id="330d7-165">Instead, transactions usually have a column that represents the transaction date, which can be used as the watermark value.</span></span> <span data-ttu-id="330d7-166">Beispielsweise enthalten in der OLTP-Datenbank von Wide World Importers die Tabellen „Sales.Invoices“ und „Sales.InvoiceLines“ das Feld `LastEditedWhen`, dessen Standardwert `sysdatetime()` ist.</span><span class="sxs-lookup"><span data-stu-id="330d7-166">For example, in the Wide World Importers OLTP database, the Sales.Invoices and Sales.InvoiceLines tables have a `LastEditedWhen` field that defaults to `sysdatetime()`.</span></span>

<span data-ttu-id="330d7-167">Hier ist der allgemeine Ablauf für die ELT-Pipeline:</span><span class="sxs-lookup"><span data-stu-id="330d7-167">Here is the general flow for the ELT pipeline:</span></span>

1. <span data-ttu-id="330d7-168">Verfolgen Sie für jede Tabelle in der Quelldatenbank den Trennzeitpunkt der Ausführung des letzten ELT-Auftrags nach.</span><span class="sxs-lookup"><span data-stu-id="330d7-168">For each table in the source database, track the cutoff time when the last ELT job ran.</span></span> <span data-ttu-id="330d7-169">Speichern Sie diese Informationen im Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="330d7-169">Store this information in the data warehouse.</span></span> <span data-ttu-id="330d7-170">(Bei der Ersteinrichtung sind alle Zeiten auf „1.1.1900“ festgelegt.)</span><span class="sxs-lookup"><span data-stu-id="330d7-170">(On initial setup, all times are set to '1-1-1900'.)</span></span>

2. <span data-ttu-id="330d7-171">Während des Datenexportschritts wird der Trennzeitpunkt als Parameter an einen Satz von gespeicherten Prozeduren in der Quelldatenbank übergeben.</span><span class="sxs-lookup"><span data-stu-id="330d7-171">During the data export step, the cutoff time is passed as a parameter to a set of stored procedures in the source database.</span></span> <span data-ttu-id="330d7-172">Diese gespeicherten Prozeduren fragen alle Datensätze ab, die nach dem Trennzeitpunkt geändert oder erstellt wurden.</span><span class="sxs-lookup"><span data-stu-id="330d7-172">These stored procedures query for any records that were changed or created after the cutoff time.</span></span> <span data-ttu-id="330d7-173">Für die Sales-Faktentabelle wird die Spalte `LastEditedWhen` verwendet.</span><span class="sxs-lookup"><span data-stu-id="330d7-173">For the Sales fact table, the `LastEditedWhen` column is used.</span></span> <span data-ttu-id="330d7-174">Für die Dimensionsdaten werden temporale Tabellen mit Systemversionsverwaltung verwendet.</span><span class="sxs-lookup"><span data-stu-id="330d7-174">For the dimension data, system-versioned temporal tables are used.</span></span>

3. <span data-ttu-id="330d7-175">Wenn die Datenmigration abgeschlossen ist, aktualisieren Sie die Tabelle, in der die Trennzeitpunkte gespeichert werden.</span><span class="sxs-lookup"><span data-stu-id="330d7-175">When the data migration is complete, update the table that stores the cutoff times.</span></span>

<span data-ttu-id="330d7-176">Es ist auch hilfreich, eine *Herkunft* für jede ELT-Ausführung aufzuzeichnen.</span><span class="sxs-lookup"><span data-stu-id="330d7-176">It's also useful to record a *lineage* for each ELT run.</span></span> <span data-ttu-id="330d7-177">Für einen bestimmten Datensatz ordnet die Herkunft diesen Datensatz der ELT-Ausführung zu, bei der die Daten erzeugt wurden.</span><span class="sxs-lookup"><span data-stu-id="330d7-177">For a given record, the lineage associates that record with the ELT run that produced the data.</span></span> <span data-ttu-id="330d7-178">Bei jeder ETL-Ausführung wird für jede Tabelle ein neuer Herkunftsdatensatz erstellt, der die Start- und Endladezeiten anzeigt.</span><span class="sxs-lookup"><span data-stu-id="330d7-178">For each ETL run, a new lineage record is created for every table, showing the starting and ending load times.</span></span> <span data-ttu-id="330d7-179">Die Herkunftsschlüssel für die einzelnen Datensätze werden in den Dimensions- und Faktentabellen gespeichert.</span><span class="sxs-lookup"><span data-stu-id="330d7-179">The lineage keys for each record are stored in the dimension and fact tables.</span></span>

![Screenshot der Tabelle „Dimension.City“](./images/city-dimension-table.png)

<span data-ttu-id="330d7-181">Aktualisieren Sie das Analysis Services-Tabellenmodell, nachdem ein neuer Datenbatch in das Warehouse geladen wurde.</span><span class="sxs-lookup"><span data-stu-id="330d7-181">After a new batch of data is loaded into the warehouse, refresh the Analysis Services tabular model.</span></span> <span data-ttu-id="330d7-182">Siehe [Asynchrones Aktualisieren mit der REST-API](/azure/analysis-services/analysis-services-async-refresh).</span><span class="sxs-lookup"><span data-stu-id="330d7-182">See [Asynchronous refresh with the REST API](/azure/analysis-services/analysis-services-async-refresh).</span></span>

## <a name="data-cleansing"></a><span data-ttu-id="330d7-183">Datenbereinigung</span><span class="sxs-lookup"><span data-stu-id="330d7-183">Data cleansing</span></span>

<span data-ttu-id="330d7-184">Die Datenbereinigung sollte Teil des ELT-Prozesses sein.</span><span class="sxs-lookup"><span data-stu-id="330d7-184">Data cleansing should be part of the ELT process.</span></span> <span data-ttu-id="330d7-185">In dieser Referenzarchitektur ist die Tabelle mit der Stadtbevölkerung eine Quelle ungültiger Daten, in der einige Städte keine Bevölkerung aufweisen, da möglicherweise keine Daten verfügbar waren.</span><span class="sxs-lookup"><span data-stu-id="330d7-185">In this reference architecture, one source of bad data is the city population table, where some cities have zero population, perhaps because no data was available.</span></span> <span data-ttu-id="330d7-186">Während der Verarbeitung entfernt die ELT-Pipeline diese Städte aus der Tabelle mit der Stadtbevölkerung.</span><span class="sxs-lookup"><span data-stu-id="330d7-186">During processing, the ELT pipeline removes those cities from the city population table.</span></span> <span data-ttu-id="330d7-187">Führen Sie die Datenbereinigung in Stagingtabellen statt in externen Tabellen durch.</span><span class="sxs-lookup"><span data-stu-id="330d7-187">Perform data cleansing on staging tables, rather than external tables.</span></span>

<span data-ttu-id="330d7-188">Hier ist die gespeicherte Prozedur, die die Städte ohne Bevölkerung der Tabelle mit der Stadtbevölkerung entfernt.</span><span class="sxs-lookup"><span data-stu-id="330d7-188">Here is the stored procedure that removes the cities with zero population from the City Population table.</span></span> <span data-ttu-id="330d7-189">(Sie finden die Quelldatei [hier](https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/citypopulation/%5BIntegration%5D.%5BMigrateExternalCityPopulationData%5D.sql).)</span><span class="sxs-lookup"><span data-stu-id="330d7-189">(You can find the source file [here](https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/citypopulation/%5BIntegration%5D.%5BMigrateExternalCityPopulationData%5D.sql).)</span></span>

```sql
DELETE FROM [Integration].[CityPopulation_Staging]
WHERE RowNumber in (SELECT DISTINCT RowNumber
FROM [Integration].[CityPopulation_Staging]
WHERE POPULATION = 0
GROUP BY RowNumber
HAVING COUNT(RowNumber) = 4)
```

## <a name="external-data-sources"></a><span data-ttu-id="330d7-190">Externe Datenquellen</span><span class="sxs-lookup"><span data-stu-id="330d7-190">External data sources</span></span>

<span data-ttu-id="330d7-191">Data Warehouses fassen häufig Daten aus mehreren Quellen zusammen.</span><span class="sxs-lookup"><span data-stu-id="330d7-191">Data warehouses often consolidate data from multiple sources.</span></span> <span data-ttu-id="330d7-192">Diese Referenzarchitektur lädt eine externe Datenquelle, die demografische Daten enthält.</span><span class="sxs-lookup"><span data-stu-id="330d7-192">This reference architecture loads an external data source that contains demographics data.</span></span> <span data-ttu-id="330d7-193">Dieses Dataset in Azure Blob Storage als Teil des Beispiels [WorldWideImportersDW](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/sample-scripts/polybase) verfügbar.</span><span class="sxs-lookup"><span data-stu-id="330d7-193">This dataset is available in Azure blob storage as part of the [WorldWideImportersDW](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/sample-scripts/polybase) sample.</span></span>

<span data-ttu-id="330d7-194">Azure Data Factory kann mithilfe des [Blob Storage-Connectors](/azure/data-factory/connector-azure-blob-storage) direkt aus Blob Storage kopieren.</span><span class="sxs-lookup"><span data-stu-id="330d7-194">Azure Data Factory can copy directly from blob storage, using the [blob storage connector](/azure/data-factory/connector-azure-blob-storage).</span></span> <span data-ttu-id="330d7-195">Der Connector erfordert jedoch eine Verbindungszeichenfolge oder eine Shared Access Signature, damit er nicht zum Kopieren eines Blobs mit öffentlichem Lesezugriff verwendet werden kann.</span><span class="sxs-lookup"><span data-stu-id="330d7-195">However, the connector requires a connection string or a shared access signature, so it can't be used to copy a blob with public read access.</span></span> <span data-ttu-id="330d7-196">Um dieses Problem zu umgehen, können Sie PolyBase zum Erstellen einer externen Tabelle über Blob Storage verwenden und die externen Tabellen dann in SQL Data Warehouse kopieren.</span><span class="sxs-lookup"><span data-stu-id="330d7-196">As a workaround, you can use PolyBase to create an external table over Blob storage and then copy the external tables into SQL Data Warehouse.</span></span>

## <a name="handling-large-binary-data"></a><span data-ttu-id="330d7-197">Verarbeiten von umfangreichen Binärdaten</span><span class="sxs-lookup"><span data-stu-id="330d7-197">Handling large binary data</span></span>

<span data-ttu-id="330d7-198">In der Quelldatenbank weist die Tabelle mit Städten die Spalte „Ort“ auf, die den räumlichen Datentyp [Geografie](/sql/t-sql/spatial-geography/spatial-types-geography) enthält.</span><span class="sxs-lookup"><span data-stu-id="330d7-198">In the source database, the Cities table has a Location column that holds a [geography](/sql/t-sql/spatial-geography/spatial-types-geography) spatial data type.</span></span> <span data-ttu-id="330d7-199">SQL Data Warehouse unterstützt den Typ **Geografie** nicht systemintern, daher wird dieses Feld während des Ladens in einen **varbinary**-Typ konvertiert.</span><span class="sxs-lookup"><span data-stu-id="330d7-199">SQL Data Warehouse doesn't support the **geography** type natively, so this field is converted to a **varbinary** type during loading.</span></span> <span data-ttu-id="330d7-200">(Siehe [Verwenden von Problemumgehungen für nicht unterstützte Datentypen](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types#unsupported-data-types).)</span><span class="sxs-lookup"><span data-stu-id="330d7-200">(See [Workarounds for unsupported data types](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types#unsupported-data-types).)</span></span>

<span data-ttu-id="330d7-201">PolyBase unterstützt jedoch eine maximale Spaltengröße von `varbinary(8000)`, was bedeutet, dass einige Daten möglicherweise abgeschnitten werden.</span><span class="sxs-lookup"><span data-stu-id="330d7-201">However, PolyBase supports a maximum column size of `varbinary(8000)`, which means some data could be truncated.</span></span> <span data-ttu-id="330d7-202">Dieses Problem lässt sich umgehen, indem die Daten während des Exports wie folgt in Blöcke aufgeteilt und dann wieder zusammengefügt werden:</span><span class="sxs-lookup"><span data-stu-id="330d7-202">A workaround for this problem is to break the data up into chunks during export, and then reassemble the chunks, as follows:</span></span>

1. <span data-ttu-id="330d7-203">Erstellen Sie eine temporäre Stagingtabelle für die Standort-Spalte.</span><span class="sxs-lookup"><span data-stu-id="330d7-203">Create a temporary staging table for the Location column.</span></span>

2. <span data-ttu-id="330d7-204">Teilen Sie die Standortdaten für jede Stadt in Blöcke von 8000 Bytes auf. Dies ergibt 1 &ndash; N Zeilen für jede Stadt.</span><span class="sxs-lookup"><span data-stu-id="330d7-204">For each city, split the location data into 8000-byte chunks, resulting in 1 &ndash; N rows for each city.</span></span>

3. <span data-ttu-id="330d7-205">Um die Blöcke wieder zusammenzusetzen, verwenden Sie den T-SQL-[PIVOT](/sql/t-sql/queries/from-using-pivot-and-unpivot)-Operator, um Zeilen in Spalten umwandeln und dann die Spaltenwerte für jede Stadt zu verketten.</span><span class="sxs-lookup"><span data-stu-id="330d7-205">To reassemble the chunks, use the T-SQL [PIVOT](/sql/t-sql/queries/from-using-pivot-and-unpivot) operator to convert rows into columns and then concatenate the column values for each city.</span></span>

<span data-ttu-id="330d7-206">Die Herausforderung besteht darin, dass jede Stadt je nach Größe der geografischen Daten in eine unterschiedliche Anzahl von Zeilen aufgeteilt wird.</span><span class="sxs-lookup"><span data-stu-id="330d7-206">The challenge is that each city will be split into a different number of rows, depending on the size of geography data.</span></span> <span data-ttu-id="330d7-207">Damit der PIVOT-Operator funktioniert, muss jede Stadt die gleiche Anzahl von Zeilen aufweisen.</span><span class="sxs-lookup"><span data-stu-id="330d7-207">For the PIVOT operator to work, every city must have the same number of rows.</span></span> <span data-ttu-id="330d7-208">Dazu wendet die T-SQL-Abfrage (die Sie [hier][MergeLocation] anzeigen können) einige Tricks an, um die Zeilen mit leeren Werten aufzufüllen, sodass jede Stadt nach dem Pivot-Vorgang die gleiche Anzahl von Spalten aufweist.</span><span class="sxs-lookup"><span data-stu-id="330d7-208">To make this work, the T-SQL query (which you can view [here][MergeLocation]) does some tricks to pad out the rows with blank values, so that every city has the same number of columns after the pivot.</span></span> <span data-ttu-id="330d7-209">Die resultierende Abfrage erweist sich als wesentlich schneller als Durchlaufen der einzelnen Zeilen.</span><span class="sxs-lookup"><span data-stu-id="330d7-209">The resulting query turns out to be much faster than looping through the rows one at a time.</span></span>

<span data-ttu-id="330d7-210">Der gleiche Ansatz wird für Bilddaten verwendet.</span><span class="sxs-lookup"><span data-stu-id="330d7-210">The same approach is used for image data.</span></span>

## <a name="slowly-changing-dimensions"></a><span data-ttu-id="330d7-211">Langsam veränderliche Dimensionen</span><span class="sxs-lookup"><span data-stu-id="330d7-211">Slowly changing dimensions</span></span>

<span data-ttu-id="330d7-212">Dimensionsdaten sind relativ statisch, können sich jedoch ändern.</span><span class="sxs-lookup"><span data-stu-id="330d7-212">Dimension data is relatively static, but it can change.</span></span> <span data-ttu-id="330d7-213">Beispielsweise kann ein Produkt einer anderen Produktkategorie zugewiesen werden.</span><span class="sxs-lookup"><span data-stu-id="330d7-213">For example, a product might get reassigned to a different product category.</span></span> <span data-ttu-id="330d7-214">Es gibt verschiedene Ansätze zur Handhabung von langsam veränderlichen Dimensionen.</span><span class="sxs-lookup"><span data-stu-id="330d7-214">There are several approaches to handling slowly changing dimensions.</span></span> <span data-ttu-id="330d7-215">Ein gängiges Verfahren mit der Bezeichnung [Typ 2](https://wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row) ist das Hinzufügen eines neuen Datensatzes bei jeder Dimensionsänderung.</span><span class="sxs-lookup"><span data-stu-id="330d7-215">A common technique, called [Type 2](https://wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row), is to add a new record whenever a dimension changes.</span></span>

<span data-ttu-id="330d7-216">Zur Umsetzung des Typ 2-Ansatzes erfordern Dimensionstabellen zusätzliche Spalten, die den effektiven Datumsbereich für einen bestimmten Datensatz angeben.</span><span class="sxs-lookup"><span data-stu-id="330d7-216">In order to implement the Type 2 approach, dimension tables need additional columns that specify the effective date range for a given record.</span></span> <span data-ttu-id="330d7-217">Außerdem werden Primärschlüssel aus der Quelldatenbank dupliziert, daher muss die Dimensionstabelle über einen künstlichen Primärschlüssel verfügen.</span><span class="sxs-lookup"><span data-stu-id="330d7-217">Also, primary keys from the source database will be duplicated, so the dimension table must have an artificial primary key.</span></span>

<span data-ttu-id="330d7-218">Die folgende Abbildung zeigt die Tabelle „Dimension.City“.</span><span class="sxs-lookup"><span data-stu-id="330d7-218">The following image shows the Dimension.City table.</span></span> <span data-ttu-id="330d7-219">Die Spalte `WWI City ID` ist der Primärschlüssel aus der Quelldatenbank.</span><span class="sxs-lookup"><span data-stu-id="330d7-219">The `WWI City ID` column is the primary key from the source database.</span></span> <span data-ttu-id="330d7-220">Die Spalte `City Key` ist ein künstlicher Schlüssel, der während der Verarbeitung der ETL-Pipeline generiert wurde.</span><span class="sxs-lookup"><span data-stu-id="330d7-220">The `City Key` column is an artificial key generated during the ETL pipeline.</span></span> <span data-ttu-id="330d7-221">Beachten Sie auch, dass in der Tabelle die Spalten `Valid From` und `Valid To` vorhanden sind, die den Bereich definieren, in dem die einzelnen Zeilen gültig waren.</span><span class="sxs-lookup"><span data-stu-id="330d7-221">Also notice that the table has `Valid From` and `Valid To` columns, which define the range when each row was valid.</span></span> <span data-ttu-id="330d7-222">Das `Valid To`-Element aktueller Werte entspricht „9999-12-31“.</span><span class="sxs-lookup"><span data-stu-id="330d7-222">Current values have a `Valid To` equal to '9999-12-31'.</span></span>

![Screenshot der Tabelle „Dimension.City“](./images/city-dimension-table.png)

<span data-ttu-id="330d7-224">Der Vorteil dieses Ansatzes ist, dass historische Daten beibehalten werden, die für die Analyse nützlich sein können.</span><span class="sxs-lookup"><span data-stu-id="330d7-224">The advantage of this approach is that it preserves historical data, which can be valuable for analysis.</span></span> <span data-ttu-id="330d7-225">Allerdings sind dabei auch mehrere Zeilen für dieselbe Entität vorhanden.</span><span class="sxs-lookup"><span data-stu-id="330d7-225">However, it also means there will be multiple rows for the same entity.</span></span> <span data-ttu-id="330d7-226">Hier sehen Sie beispielsweise Datensätze, die `WWI City ID` = 28561 entsprechen:</span><span class="sxs-lookup"><span data-stu-id="330d7-226">For example, here are the records that match `WWI City ID` = 28561:</span></span>

![Zweiter Screenshot der Tabelle „Dimension.City“](./images/city-dimension-table-2.png)

<span data-ttu-id="330d7-228">Ordnen Sie jeden Sales-Fakt einer einzelnen Zeile in der Tabelle „Dimension.City“ zu, die dem Rechnungsdatum entspricht.</span><span class="sxs-lookup"><span data-stu-id="330d7-228">For each Sales fact, you want to associate that fact with a single row in City dimension table, corresponding to the invoice date.</span></span> <span data-ttu-id="330d7-229">Erstellen Sie als Teil des ETL-Prozesses eine weitere Spalte, die</span><span class="sxs-lookup"><span data-stu-id="330d7-229">As part of the ETL process, create an additional column that</span></span> 

<span data-ttu-id="330d7-230">Die folgende T-SQL-Abfrage erstellt eine temporäre Tabelle, die jede Rechnung dem richtigen City-Schlüssel aus der Tabelle „Dimension.City“ zuordnet.</span><span class="sxs-lookup"><span data-stu-id="330d7-230">The following T-SQL query creates a temporary table that associates each invoice with the correct City Key from the City dimension table.</span></span>

```sql
CREATE TABLE CityHolder
WITH (HEAP , DISTRIBUTION = HASH([WWI Invoice ID]))
AS
SELECT DISTINCT s1.[WWI Invoice ID] AS [WWI Invoice ID],
                c.[City Key] AS [City Key]
    FROM [Integration].[Sale_Staging] s1
    CROSS APPLY (
                SELECT TOP 1 [City Key]
                    FROM [Dimension].[City]
                WHERE [WWI City ID] = s1.[WWI City ID]
                    AND s1.[Last Modified When] > [Valid From]
                    AND s1.[Last Modified When] <= [Valid To]
                ORDER BY [Valid From], [City Key] DESC
                ) c

```

<span data-ttu-id="330d7-231">Diese Tabelle wird verwendet, um eine Spalte in der Sales-Faktentabelle auszufüllen:</span><span class="sxs-lookup"><span data-stu-id="330d7-231">This table is used to populate a column in the Sales fact table:</span></span>

```sql
UPDATE [Integration].[Sale_Staging]
SET [Integration].[Sale_Staging].[WWI Customer ID] =  CustomerHolder.[WWI Customer ID]
```

<span data-ttu-id="330d7-232">Diese Spalte ermöglicht, dass eine Power BI-Abfrage den richtigen City-Datensatz für eine bestimmte Verkaufsrechnung findet.</span><span class="sxs-lookup"><span data-stu-id="330d7-232">This column enables a Power BI query to find the correct City record for a given sales invoice.</span></span>

## <a name="security-considerations"></a><span data-ttu-id="330d7-233">Sicherheitshinweise</span><span class="sxs-lookup"><span data-stu-id="330d7-233">Security considerations</span></span>

<span data-ttu-id="330d7-234">Für höhere Sicherheit können Sie [Virtual Network-Dienstendpunkte](/azure/virtual-network/virtual-network-service-endpoints-overview) zum Schützen von Azure-Dienstressourcen verwenden, indem diese ausschließlich auf Ihr virtuelles Netzwerk beschränkt sind.</span><span class="sxs-lookup"><span data-stu-id="330d7-234">For additional security, you can use [Virtual Network service endpoints](/azure/virtual-network/virtual-network-service-endpoints-overview) to secure Azure service resources to only your virtual network.</span></span> <span data-ttu-id="330d7-235">Der öffentliche Internetzugriff auf diese Ressourcen wird dadurch vollständig entfernt, sodass nur Datenverkehr aus Ihrem virtuellen Netzwerk zulässig ist.</span><span class="sxs-lookup"><span data-stu-id="330d7-235">This fully removes public Internet access to those resources, allowing traffic only from your virtual network.</span></span>

<span data-ttu-id="330d7-236">Mit diesem Ansatz erstellen Sie ein VNET in Azure und dann private Dienstendpunkte für Azure-Dienste.</span><span class="sxs-lookup"><span data-stu-id="330d7-236">With this approach, you create a VNet in Azure and then create private service endpoints for Azure services.</span></span> <span data-ttu-id="330d7-237">Diese Dienste sind dann auf Datenverkehr aus diesem virtuellen Netzwerk beschränkt.</span><span class="sxs-lookup"><span data-stu-id="330d7-237">Those services are then restricted to traffic from that virtual network.</span></span> <span data-ttu-id="330d7-238">Sie können auch über ein Gateway aus Ihrem lokalen Netzwerk darauf zugreifen.</span><span class="sxs-lookup"><span data-stu-id="330d7-238">You can also reach them from your on-premises network through a gateway.</span></span>

<span data-ttu-id="330d7-239">Bedenken Sie dabei folgende Einschränkungen:</span><span class="sxs-lookup"><span data-stu-id="330d7-239">Be aware of the following limitations:</span></span>

- <span data-ttu-id="330d7-240">Zum Zeitpunkt der Erstellung dieser Referenzarchitektur werden VNET-Dienstendpunkte für Azure Storage und Azure SQL Data Warehouse, aber nicht für Azure Analysis Services unterstützt.</span><span class="sxs-lookup"><span data-stu-id="330d7-240">At the time this reference architecture was created, VNet service endpoints are supported for Azure Storage and Azure SQL Data Warehouse, but not for Azure Analysis Service.</span></span> <span data-ttu-id="330d7-241">Überprüfen Sie den aktuellen Status [hier](https://azure.microsoft.com/updates/?product=virtual-network).</span><span class="sxs-lookup"><span data-stu-id="330d7-241">Check the latest status [here](https://azure.microsoft.com/updates/?product=virtual-network).</span></span>

- <span data-ttu-id="330d7-242">Wenn Dienstendpunkte für Azure Storage aktiviert sind, kann PolyBase keine Daten aus Storage in SQL Data Warehouse kopieren.</span><span class="sxs-lookup"><span data-stu-id="330d7-242">If service endpoints are enabled for Azure Storage, PolyBase cannot copy data from Storage into SQL Data Warehouse.</span></span> <span data-ttu-id="330d7-243">Dieses Problem kann entschärft werden.</span><span class="sxs-lookup"><span data-stu-id="330d7-243">There is a mitigation for this issue.</span></span> <span data-ttu-id="330d7-244">Weitere Informationen finden Sie unter [Auswirkungen der Verwendung von VNET-Dienstendpunkten mit Azure Storage](/azure/sql-database/sql-database-vnet-service-endpoint-rule-overview?toc=%2fazure%2fvirtual-network%2ftoc.json#impact-of-using-vnet-service-endpoints-with-azure-storage).</span><span class="sxs-lookup"><span data-stu-id="330d7-244">For more information, see [Impact of using VNet Service Endpoints with Azure storage](/azure/sql-database/sql-database-vnet-service-endpoint-rule-overview?toc=%2fazure%2fvirtual-network%2ftoc.json#impact-of-using-vnet-service-endpoints-with-azure-storage).</span></span>

- <span data-ttu-id="330d7-245">Zum Verschieben von Daten aus einem lokalen Speicher in Azure Storage müssen Sie öffentliche IP-Adressen in Ihrem lokalen Speicher oder ExpressRoute auf die Whitelist setzen.</span><span class="sxs-lookup"><span data-stu-id="330d7-245">To move data from on-premises into Azure Storage, you will need to whitelist public IP addresses from your on-premises or ExpressRoute.</span></span> <span data-ttu-id="330d7-246">Details finden Sie unter [Schützen von Azure-Diensten in virtuellen Netzwerken](/azure/virtual-network/virtual-network-service-endpoints-overview#securing-azure-services-to-virtual-networks).</span><span class="sxs-lookup"><span data-stu-id="330d7-246">For details, see [Securing Azure services to virtual networks](/azure/virtual-network/virtual-network-service-endpoints-overview#securing-azure-services-to-virtual-networks).</span></span>

- <span data-ttu-id="330d7-247">Stellen Sie im virtuellen Netzwerk einen virtuellen Windows-Computer bereit, der den SQL Data Warehouse-Dienstendpunkt enthält, damit Analysis Services Daten aus SQL Data Warehouse lesen kann.</span><span class="sxs-lookup"><span data-stu-id="330d7-247">To enable Analysis Services to read data from SQL Data Warehouse, deploy a Windows VM to the virtual network that contains the SQL Data Warehouse service endpoint.</span></span> <span data-ttu-id="330d7-248">Installieren Sie auf diesem virtuellen Computer ein [lokales Azure-Datengateway](/azure/analysis-services/analysis-services-gateway).</span><span class="sxs-lookup"><span data-stu-id="330d7-248">Install [Azure On-premises Data Gateway](/azure/analysis-services/analysis-services-gateway) on this VM.</span></span> <span data-ttu-id="330d7-249">Verbinden Sie dann Ihren Azure Analysis-Dienst, mit dem Datengateway.</span><span class="sxs-lookup"><span data-stu-id="330d7-249">Then connect your Azure Analysis service to the data gateway.</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="330d7-250">Bereitstellen der Lösung</span><span class="sxs-lookup"><span data-stu-id="330d7-250">Deploy the solution</span></span>

<span data-ttu-id="330d7-251">Führen Sie zum Bereitstellen und Ausführen der Referenzimplementierung die Schritte in der [GitHub-Infodatei][github] aus.</span><span class="sxs-lookup"><span data-stu-id="330d7-251">To the deploy and run the reference implementation, follow the steps in the [GitHub readme][github].</span></span> <span data-ttu-id="330d7-252">Folgendes wird bereitgestellt:</span><span class="sxs-lookup"><span data-stu-id="330d7-252">It deploys the following:</span></span>

- <span data-ttu-id="330d7-253">Eine Windows-VM, um einen lokalen Datenbankserver zu simulieren.</span><span class="sxs-lookup"><span data-stu-id="330d7-253">A Windows VM to simulate an on-premises database server.</span></span> <span data-ttu-id="330d7-254">Sie enthält SQL Server 2017 und zugehörige Tools zusammen mit Power BI Desktop.</span><span class="sxs-lookup"><span data-stu-id="330d7-254">It includes SQL Server 2017 and related tools, along with Power BI Desktop.</span></span>
- <span data-ttu-id="330d7-255">Ein Azure Storage-Konto, das Blobspeicher zum Speichern von Daten bereitstellt, die aus SQL Server-Datenbank exportiert wurden.</span><span class="sxs-lookup"><span data-stu-id="330d7-255">An Azure storage account that provides Blob storage to hold data exported from the SQL Server database.</span></span>
- <span data-ttu-id="330d7-256">Eine Instanz von Azure SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="330d7-256">An Azure SQL Data Warehouse instance.</span></span>
- <span data-ttu-id="330d7-257">Eine Azure Analysis Services-Instanz.</span><span class="sxs-lookup"><span data-stu-id="330d7-257">An Azure Analysis Services instance.</span></span>
- <span data-ttu-id="330d7-258">Azure Data Factory und die Data Factory-Pipeline für den ELT-Auftrag.</span><span class="sxs-lookup"><span data-stu-id="330d7-258">Azure Data Factory and the Data Factory pipeline for the ELT job.</span></span>

## <a name="related-resources"></a><span data-ttu-id="330d7-259">Zugehörige Ressourcen</span><span class="sxs-lookup"><span data-stu-id="330d7-259">Related resources</span></span>

<span data-ttu-id="330d7-260">Es wird empfohlen, sich das folgende [Azure-Beispielszenario](/azure/architecture/example-scenario) anzusehen. Darin wird veranschaulicht, wie einige dieser Technologien in spezifischen Lösungen verwendet werden:</span><span class="sxs-lookup"><span data-stu-id="330d7-260">You may want to review the following [Azure example scenarios](/azure/architecture/example-scenario) that demonstrate specific solutions using some of the same technologies:</span></span>

- [<span data-ttu-id="330d7-261">Data Warehousing und Analysen für Vertrieb und Marketing</span><span class="sxs-lookup"><span data-stu-id="330d7-261">Data warehousing and analytics for sales and marketing</span></span>](/azure/architecture/example-scenario/data/data-warehouse)
- [<span data-ttu-id="330d7-262">ETL-Hybridvorgänge mit vorhandenen lokalen SSIS-Bereitstellungen und Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="330d7-262">Hybrid ETL with existing on-premises SSIS and Azure Data Factory</span></span>](/azure/architecture/example-scenario/data/hybrid-etl-with-adf)

<!-- links -->

[adf]: /azure/data-factory
[github]: https://github.com/mspnp/azure-data-factory-sqldw-elt-pipeline
[MergeLocation]: https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/city/%5BIntegration%5D.%5BMergeLocation%5D.sql
[wwi]: /sql/sample/world-wide-importers/wide-world-importers-oltp-database
