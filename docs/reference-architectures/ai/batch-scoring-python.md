---
title: Batchbewertung von Python-Modellen in Azure
description: Erstellen Sie mit Azure Batch AI eine skalierbare Lösung für die parallele Batchbewertung von Modellen nach einem Zeitplan.
author: njray
ms.date: 12/13/18
ms.custom: azcat-ai
ms.openlocfilehash: 93fc0c81663931c0a8b0f54b41934287056e6953
ms.sourcegitcommit: fb22348f917a76e30a6c090fcd4a18decba0b398
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 12/16/2018
ms.locfileid: "53450824"
---
# <a name="batch-scoring-of-python-models-on-azure"></a><span data-ttu-id="a5475-103">Batchbewertung von Python-Modellen in Azure</span><span class="sxs-lookup"><span data-stu-id="a5475-103">Batch scoring of Python models on Azure</span></span>

<span data-ttu-id="a5475-104">Diese Referenzarchitektur veranschaulicht, wie Sie mit Azure Batch AI eine skalierbare Lösung für die parallele Batchbewertung vieler Modelle nach einem Zeitplan erstellen.</span><span class="sxs-lookup"><span data-stu-id="a5475-104">This reference architecture shows how to build a scalable solution for batch scoring many models on a schedule in parallel using Azure Batch AI.</span></span> <span data-ttu-id="a5475-105">Die Lösung kann als Vorlage genutzt und für unterschiedliche Probleme generalisiert werden.</span><span class="sxs-lookup"><span data-stu-id="a5475-105">The solution can be used as a template and can generalize to different problems.</span></span>

<span data-ttu-id="a5475-106">Eine Referenzimplementierung für diese Architektur ist auf [GitHub][github] verfügbar.</span><span class="sxs-lookup"><span data-stu-id="a5475-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Batchbewertung von Python-Modellen in Azure](./_images/batch-scoring-python.png)

<span data-ttu-id="a5475-108">**Szenario:** Diese Lösung überwacht den Betrieb einer großen Zahl von Geräten unter einer IoT-Einstellung, wobei jedes Gerät fortlaufend Sensormesswerte sendet.</span><span class="sxs-lookup"><span data-stu-id="a5475-108">**Scenario**: This solution monitors the operation of a large number of devices in an IoT setting where each device sends sensor readings continuously.</span></span> <span data-ttu-id="a5475-109">Es wird vorausgesetzt, dass jedes Gerät über vorab trainierte Modelle für die Anomalieerkennung verfügt, mit der Folgendes vorhergesagt werden kann: Entspricht eine Reihe von Messwerten, die für ein vordefiniertes Zeitintervall aggregiert werden, einer Anomalie?</span><span class="sxs-lookup"><span data-stu-id="a5475-109">Each device is assumed to have pre-trained anomaly detection models that need to be used to predict whether a series of measurements, that are aggregated over a predefined time interval, correspond to an anomaly or not.</span></span> <span data-ttu-id="a5475-110">In der Praxis kann dies ein Datenstrom mit Sensormesswerten sein, die gefiltert und aggregiert werden müssen, bevor sie für das Training oder für Echtzeitbewertungen verwendet werden.</span><span class="sxs-lookup"><span data-stu-id="a5475-110">In real-world scenarios, this could be a stream of sensor readings that need to be filtered and aggregated before being used in training or real-time scoring.</span></span> <span data-ttu-id="a5475-111">Der Einfachheit halber wird für die Lösung beim Ausführen von Bewertungsaufträgen dieselbe Datendatei verwendet.</span><span class="sxs-lookup"><span data-stu-id="a5475-111">For simplicity, the solution uses the same data file when executing scoring jobs.</span></span>

## <a name="architecture"></a><span data-ttu-id="a5475-112">Architektur</span><span class="sxs-lookup"><span data-stu-id="a5475-112">Architecture</span></span>

<span data-ttu-id="a5475-113">Diese Architektur umfasst die folgenden Komponenten:</span><span class="sxs-lookup"><span data-stu-id="a5475-113">This architecture consists of the following components:</span></span>

<span data-ttu-id="a5475-114">[Azure Event Hubs][event-hubs]:</span><span class="sxs-lookup"><span data-stu-id="a5475-114">[Azure Event Hubs][event-hubs].</span></span> <span data-ttu-id="a5475-115">Mit diesem Dienst für die Nachrichtenerfassung können Millionen von Ereignismeldungen pro Sekunde erfasst werden.</span><span class="sxs-lookup"><span data-stu-id="a5475-115">This message ingestion service can ingest millions of event messages per second.</span></span> <span data-ttu-id="a5475-116">In dieser Architektur senden Sensoren einen Datenstrom an den Event Hub.</span><span class="sxs-lookup"><span data-stu-id="a5475-116">In this architecture, sensors send a stream of data to the event hub.</span></span>

<span data-ttu-id="a5475-117">[Azure Stream Analytics][stream-analytics]:</span><span class="sxs-lookup"><span data-stu-id="a5475-117">[Azure Stream Analytics][stream-analytics].</span></span> <span data-ttu-id="a5475-118">Ein Modul für die Ereignisverarbeitung.</span><span class="sxs-lookup"><span data-stu-id="a5475-118">An event-processing engine.</span></span> <span data-ttu-id="a5475-119">Ein Stream Analytics-Auftrag liest die Datenströme aus dem Event Hub und führt die Datenstromverarbeitung durch.</span><span class="sxs-lookup"><span data-stu-id="a5475-119">A Stream Analytics job reads the data streams from the event hub and performs stream processing.</span></span>

<span data-ttu-id="a5475-120">[Azure Batch AI][batch-ai]:</span><span class="sxs-lookup"><span data-stu-id="a5475-120">[Azure Batch AI][batch-ai].</span></span> <span data-ttu-id="a5475-121">Dieses Modul für verteiltes Computing wird verwendet, um Machine Learning- und AI-Modelle in Azure bedarfsabhängig zu trainieren und zu testen.</span><span class="sxs-lookup"><span data-stu-id="a5475-121">This distributed computing engine is used to train and test machine learning and AI models at scale in Azure.</span></span> <span data-ttu-id="a5475-122">Mit Batch AI werden virtuelle Computer nach Bedarf mit einer Option für die automatische Skalierung erstellt, wobei auf jedem Knoten im Batch AI-Cluster ein Bewertungsauftrag für einen bestimmten Sensor ausgeführt wird.</span><span class="sxs-lookup"><span data-stu-id="a5475-122">Batch AI creates virtual machines on demand with an automatic scaling option, where each node in the Batch AI cluster runs a scoring job for a specific sensor.</span></span> <span data-ttu-id="a5475-123">Das [Python-Skript][python-script] für die Bewertung wird in Docker-Containern ausgeführt, die auf jedem Knoten des Clusters erstellt werden. Hiermit werden die relevanten Sensordaten gelesen und Vorhersagen generiert und im Blobspeicher gespeichert.</span><span class="sxs-lookup"><span data-stu-id="a5475-123">The scoring Python [script][python-script] runs in Docker containers that are created on each node of the cluster, where it reads the relevant sensor data, generates predictions and stores them in Blob storage.</span></span>

<span data-ttu-id="a5475-124">[Azure Blob Storage][storage]:</span><span class="sxs-lookup"><span data-stu-id="a5475-124">[Azure Blob Storage][storage].</span></span> <span data-ttu-id="a5475-125">Blobcontainer werden zum Speichern der vorab trainierten Modelle, der Daten und der Ausgabevorhersagen verwendet.</span><span class="sxs-lookup"><span data-stu-id="a5475-125">Blob containers are used to store the pretrained models, the data, and the output predictions.</span></span> <span data-ttu-id="a5475-126">Die Modelle werden im Notebook [create\_resources.ipynb][create-resources] in den Blobspeicher hochgeladen.</span><span class="sxs-lookup"><span data-stu-id="a5475-126">The models are uploaded to Blob storage in the [create\_resources.ipynb][create-resources] notebook.</span></span> <span data-ttu-id="a5475-127">Diese Modelle vom Typ [Einklassige SVM][one-class-svm] werden mit Daten trainiert, die Werte unterschiedlicher Sensoren für unterschiedliche Geräte repräsentieren.</span><span class="sxs-lookup"><span data-stu-id="a5475-127">These [one-class SVM][one-class-svm] models are trained on data that represents values of different sensors for different devices.</span></span> <span data-ttu-id="a5475-128">Bei dieser Lösung wird davon ausgegangen, dass die Datenwerte während eines festen Zeitintervalls aggregiert werden.</span><span class="sxs-lookup"><span data-stu-id="a5475-128">This solution assumes that the data values are aggregated over a fixed interval of time.</span></span>

<span data-ttu-id="a5475-129">[Azure Logic Apps][logic-apps]:</span><span class="sxs-lookup"><span data-stu-id="a5475-129">[Azure Logic Apps][logic-apps].</span></span> <span data-ttu-id="a5475-130">Mit dieser Lösung wird eine Logik-App erstellt, mit der stündliche Batch AI-Aufträge ausgeführt werden.</span><span class="sxs-lookup"><span data-stu-id="a5475-130">This solution creates a Logic App that runs hourly Batch AI jobs.</span></span> <span data-ttu-id="a5475-131">Logic Apps stellt eine einfache Möglichkeit zum Erstellen des Runtimeworkflows und der Planung für die Lösung dar.</span><span class="sxs-lookup"><span data-stu-id="a5475-131">Logic Apps provides an easy way to create the runtime workflow and scheduling for the solution.</span></span> <span data-ttu-id="a5475-132">Die Batch AI-Aufträge werden mit einem Python-[Skript][script] übermittelt, das ebenfalls in einem Docker-Container ausgeführt wird.</span><span class="sxs-lookup"><span data-stu-id="a5475-132">The Batch AI jobs are submitted using a Python [script][script] that also runs in a Docker container.</span></span>

<span data-ttu-id="a5475-133">[Azure Container Registry][acr]:</span><span class="sxs-lookup"><span data-stu-id="a5475-133">[Azure Container Registry][acr].</span></span> <span data-ttu-id="a5475-134">Docker-Images werden sowohl in Batch AI als auch in Logic Apps verwendet und im Notebook [create\_resources.ipynb][create-resources] erstellt und dann per Pushvorgang an Container Registry übertragen.</span><span class="sxs-lookup"><span data-stu-id="a5475-134">Docker images are used in both Batch AI and Logic Apps and are created in the [create\_resources.ipynb][create-resources] notebook, then pushed to Container Registry.</span></span> <span data-ttu-id="a5475-135">Dies ist eine bequeme Möglichkeit zum Hosten von Images und Instanziieren von Containern über andere Azure-Dienste (bei dieser Lösung sind dies Logic Apps und Batch AI).</span><span class="sxs-lookup"><span data-stu-id="a5475-135">This provides a convenient way to host images and instantiate containers through other Azure services—Logic Apps and Batch AI in this solution.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="a5475-136">Überlegungen zur Leistung</span><span class="sxs-lookup"><span data-stu-id="a5475-136">Performance considerations</span></span>

<span data-ttu-id="a5475-137">Für Python-Standardmodelle sind CPUs, die zum Bewältigen der Workload ausreichen, in der Regel akzeptabel.</span><span class="sxs-lookup"><span data-stu-id="a5475-137">For standard Python models, it's generally accepted that CPUs are sufficient to handle the workload.</span></span> <span data-ttu-id="a5475-138">In dieser Architektur werden CPUs verwendet.</span><span class="sxs-lookup"><span data-stu-id="a5475-138">This architecture uses CPUs.</span></span> <span data-ttu-id="a5475-139">Bei [Deep Learning-Workloads][deep] bieten GPUs aber eine deutlich bessere Leistung als CPUs. In der Regel wird ein großer Cluster mit CPUs benötigt, um eine vergleichbare Leistung zu erzielen.</span><span class="sxs-lookup"><span data-stu-id="a5475-139">However, for [deep learning workloads][deep], GPUs generally outperform CPUs by a considerable amount—a sizeable cluster of CPUs is usually needed to get comparable performance.</span></span>

### <a name="parallelizing-across-vms-vs-cores"></a><span data-ttu-id="a5475-140">Vergleich zwischen Parallelisieren auf VMs und Kernen</span><span class="sxs-lookup"><span data-stu-id="a5475-140">Parallelizing across VMs vs cores</span></span>

<span data-ttu-id="a5475-141">Beim Ausführen von Bewertungsprozessen für viele Modelle im Batchmodus müssen die Aufträge VM-übergreifend parallelisiert werden.</span><span class="sxs-lookup"><span data-stu-id="a5475-141">When running scoring processes of many models in batch mode, the jobs need to be parallelized across VMs.</span></span> <span data-ttu-id="a5475-142">Zwei Ansätze sind möglich:</span><span class="sxs-lookup"><span data-stu-id="a5475-142">Two approaches are possible:</span></span> 

* <span data-ttu-id="a5475-143">Erstellen eines größeren Clusters mit kostengünstigen VMs</span><span class="sxs-lookup"><span data-stu-id="a5475-143">Create a larger cluster using low-cost VMs.</span></span>

* <span data-ttu-id="a5475-144">Erstellen eines kleineren Clusters mit Hochleistungs-VMs, die jeweils über mehr Kerne verfügen</span><span class="sxs-lookup"><span data-stu-id="a5475-144">Create a smaller cluster using high performing VMs with more cores available on each.</span></span>

<span data-ttu-id="a5475-145">Im Allgemeinen ist das Bewerten von Python-Standardmodellen nicht so anspruchsvoll wie das Bewerten von Deep Learning-Modellen. Mit einem kleinen Cluster sollte es möglich sein, eine große Zahl von in der Warteschlange befindlichen Modellen effizient zu verarbeiten.</span><span class="sxs-lookup"><span data-stu-id="a5475-145">In general, scoring of standard Python models is not as demanding as scoring of deep learning models, and a small cluster should be able to handle a large number of queued models efficiently.</span></span> <span data-ttu-id="a5475-146">Sie können die Anzahl von Clusterknoten erhöhen, wenn die Größe der Datasets zunimmt.</span><span class="sxs-lookup"><span data-stu-id="a5475-146">You can increase the number of cluster nodes as the dataset sizes increase.</span></span>

<span data-ttu-id="a5475-147">Der Einfachheit halber wird in diesem Szenario eine Bewertungsaufgabe innerhalb von nur einem Batch AI-Auftrag übermittelt.</span><span class="sxs-lookup"><span data-stu-id="a5475-147">For convenience in this scenario, one scoring task is submitted within a single Batch AI job.</span></span> <span data-ttu-id="a5475-148">Es kann aber auch effizienter sein, mehrere Datenblöcke innerhalb desselben Batch AI-Auftrags zu bewerten.</span><span class="sxs-lookup"><span data-stu-id="a5475-148">However, it can be more efficient to score multiple data chunks within the same Batch AI job.</span></span> <span data-ttu-id="a5475-149">Schreiben Sie in diesen Fällen benutzerdefinierten Code, mit dem mehrere Datasets eingelesen werden und das entsprechende Bewertungsskript während einer Ausführung des Batch AI-Auftrags ausgeführt wird.</span><span class="sxs-lookup"><span data-stu-id="a5475-149">In those cases, write custom code to read in multiple datasets and execute the scoring script for those during a single Batch AI job execution.</span></span>

### <a name="file-servers"></a><span data-ttu-id="a5475-150">Dateiserver</span><span class="sxs-lookup"><span data-stu-id="a5475-150">File servers</span></span>

<span data-ttu-id="a5475-151">Wenn Sie Batch AI verwenden, können Sie je nach Durchsatz, der für Ihr Szenario erforderlich ist, mehrere Speicheroptionen auswählen.</span><span class="sxs-lookup"><span data-stu-id="a5475-151">When using Batch AI, you can choose multiple storage options depending on the throughput needed for your scenario.</span></span> <span data-ttu-id="a5475-152">Für Workloads mit geringem Durchsatz sollte die Verwendung von Blob Storage ausreichen.</span><span class="sxs-lookup"><span data-stu-id="a5475-152">For workloads with low throughput requirements, using blob storage should be enough.</span></span> <span data-ttu-id="a5475-153">Alternativ unterstützt Batch AI auch einen [Batch AI-Dateiserver][bai-file-server] – ein verwaltetes NFS mit nur einem Knoten –, der automatisch auf Clusterknoten bereitgestellt werden kann, um für Aufträge einen zentral zugänglichen Speicherort zur Verfügung zu stellen.</span><span class="sxs-lookup"><span data-stu-id="a5475-153">Alternatively, Batch AI also supports a [Batch AI File Server][bai-file-server], a managed, single-node NFS, which can be automatically mounted on cluster nodes to provide a centrally accessible storage location for jobs.</span></span> <span data-ttu-id="a5475-154">In der Regel wird in einem Arbeitsbereich nur ein Dateiserver benötigt. Sie können dann Daten für Ihre Trainingsaufträge in verschiedenen Verzeichnissen speichern.</span><span class="sxs-lookup"><span data-stu-id="a5475-154">For most cases, only one file server is needed in a workspace, and you can separate data for your training jobs into different directories.</span></span>

<span data-ttu-id="a5475-155">Wenn das NFS mit nur einem Knoten für Ihre Workloads nicht geeignet ist, können Sie auch andere unterstützte Speicheroptionen von Batch AI nutzen, z.B. [Azure Files][azure-files] und benutzerdefinierte Lösungen wie ein Gluster- oder Lustre-Dateisystem.</span><span class="sxs-lookup"><span data-stu-id="a5475-155">If a single-node NFS isn't appropriate for your workloads, Batch AI supports other storage options, including [Azure Files][azure-files] and custom solutions such as a Gluster or Lustre file system.</span></span>

## <a name="management-considerations"></a><span data-ttu-id="a5475-156">Überlegungen zur Verwaltung</span><span class="sxs-lookup"><span data-stu-id="a5475-156">Management considerations</span></span>

### <a name="monitoring-batch-ai-jobs"></a><span data-ttu-id="a5475-157">Überwachen von Batch AI-Aufträgen</span><span class="sxs-lookup"><span data-stu-id="a5475-157">Monitoring Batch AI jobs</span></span>

<span data-ttu-id="a5475-158">Es ist wichtig, den Status von ausgeführten Aufträgen zu überwachen, aber es kann eine ziemliche Herausforderung darstellen, einen gesamten Cluster mit aktiven Knoten zu überwachen.</span><span class="sxs-lookup"><span data-stu-id="a5475-158">It's important to monitor the progress of running jobs, but it can be a challenge to monitor across a cluster of active nodes.</span></span> <span data-ttu-id="a5475-159">Um einen Eindruck vom Gesamtzustand des Clusters zu bekommen, navigieren Sie im [Azure-Portal][portal] zum Blatt **Batch AI**, um den Zustand der Knoten im Cluster zu überprüfen.</span><span class="sxs-lookup"><span data-stu-id="a5475-159">To get a sense of the overall state of the cluster, go to the **Batch AI** blade of the [Azure Portal][portal] to inspect the state of the nodes in the cluster.</span></span> <span data-ttu-id="a5475-160">Wenn ein Knoten inaktiv oder ein Auftrag fehlgeschlagen ist, werden die Fehlerprotokolle in Blob Storage gespeichert und sind auch im Azure-Portal auf dem Blatt **Aufträge** verfügbar.</span><span class="sxs-lookup"><span data-stu-id="a5475-160">If a node is inactive or a job has failed, the error logs are saved to blob storage, and are also accessible in the **Jobs** blade of the portal.</span></span>

<span data-ttu-id="a5475-161">Verbinden Sie zum Durchführen einer umfassenderen Überwachung Protokolle mit [Application Insights][ai], oder führen Sie separate Prozesse aus, um den Status des Batch AI-Clusters und der zugehörigen Aufträge abzufragen.</span><span class="sxs-lookup"><span data-stu-id="a5475-161">For richer monitoring, connect logs to [Application Insights][ai], or run separate processes to poll for the state of the Batch AI cluster and its jobs.</span></span>

### <a name="logging-in-batch-ai"></a><span data-ttu-id="a5475-162">Protokollierung in Batch AI</span><span class="sxs-lookup"><span data-stu-id="a5475-162">Logging in Batch AI</span></span>

<span data-ttu-id="a5475-163">Batch AI protokolliert alle stdout/stderr-Vorgänge für das zugeordnete Azure-Speicherkonto.</span><span class="sxs-lookup"><span data-stu-id="a5475-163">Batch AI logs all stdout/stderr to the associated Azure storage account.</span></span> <span data-ttu-id="a5475-164">Verwenden Sie ein Speichernavigationstool, z.B. [Azure Storage-Explorer][explorer], um die Navigation durch die Protokolldateien zu vereinfachen.</span><span class="sxs-lookup"><span data-stu-id="a5475-164">For easy navigation of the log files, use a storage navigation tool such as [Azure Storage Explorer][explorer].</span></span>

<span data-ttu-id="a5475-165">Beim Bereitstellen dieser Referenzarchitektur haben Sie die Möglichkeit, ein einfacheres Protokollierungssystem einzurichten.</span><span class="sxs-lookup"><span data-stu-id="a5475-165">When you deploy this reference architecture, you have the option to set up a simpler logging system.</span></span> <span data-ttu-id="a5475-166">Mit dieser Option werden alle Protokolle für die unterschiedlichen Aufträge in demselben Verzeichnis Ihres Blobcontainers gespeichert. Dies ist unten dargestellt.</span><span class="sxs-lookup"><span data-stu-id="a5475-166">With this option, all the logs across the different jobs are saved to the same directory in your blob container as shown below.</span></span> <span data-ttu-id="a5475-167">Überwachen Sie anhand dieser Protokolle, wie lange die Verarbeitung der einzelnen Aufträge und Images dauert. So können Sie besser verstehen, wie der Prozess optimiert werden kann.</span><span class="sxs-lookup"><span data-stu-id="a5475-167">Use these logs to monitor how long it takes for each job and each image to process, so you have a better sense of how to optimize the process.</span></span>

![Azure Storage-Explorer](./_images/batch-scoring-python-monitor.png)

## <a name="cost-considerations"></a><span data-ttu-id="a5475-169">Kostenbetrachtung</span><span class="sxs-lookup"><span data-stu-id="a5475-169">Cost considerations</span></span>

<span data-ttu-id="a5475-170">Die teuersten Komponenten, die in dieser Referenzarchitektur genutzt werden, sind die Computeressourcen.</span><span class="sxs-lookup"><span data-stu-id="a5475-170">The most expensive components used in this reference architecture are the compute resources.</span></span>

<span data-ttu-id="a5475-171">Die Größe des Batch AI-Clusters wird je nach den Aufträgen in der Warteschlange zentral hoch- und herunterskaliert.</span><span class="sxs-lookup"><span data-stu-id="a5475-171">The Batch AI cluster size scales up and down depending on the jobs in the queue.</span></span> <span data-ttu-id="a5475-172">Sie haben zwei Möglichkeiten, wie Sie die [automatische Skalierung][automatic-scaling] mit Batch AI aktivieren können.</span><span class="sxs-lookup"><span data-stu-id="a5475-172">You can enable [automatic scaling][automatic-scaling] with Batch AI in one of two ways.</span></span> <span data-ttu-id="a5475-173">Bei der programmgesteuerten Variante wird sie in der ENV-Datei konfiguriert, die Teil der [Bereitstellungsschritte][github] ist. Alternativ können Sie die Skalierungsformel direkt im Portal ändern, nachdem der Cluster erstellt wurde.</span><span class="sxs-lookup"><span data-stu-id="a5475-173">You can do so programmatically, which can be configured in the .env file that is part of the [deployment steps][github], or you can change the scale formula directly in the portal after the cluster is created.</span></span>

<span data-ttu-id="a5475-174">Konfigurieren Sie für Aufträge, die nicht direkt verarbeitet werden müssen, die Formel für die automatische Skalierung so, dass der Standardzustand (Minimum) ein Cluster von null Knoten ist.</span><span class="sxs-lookup"><span data-stu-id="a5475-174">For work that doesn't require immediate processing, configure the automatic scaling formula so the default state (minimum) is a cluster of zero nodes.</span></span> <span data-ttu-id="a5475-175">Bei dieser Konfiguration hat der Cluster anfangs null Knoten. Er skaliert nur dann zentral hoch, wenn er Aufträge in der Warteschlange erkennt.</span><span class="sxs-lookup"><span data-stu-id="a5475-175">With this configuration, the cluster starts with zero nodes and only scales up when it detects jobs in the queue.</span></span> <span data-ttu-id="a5475-176">Wenn die Batchbewertung maximal einige Male am Tag ausgeführt wird, können Sie mithilfe dieser Einstellung erheblich Kosten sparen.</span><span class="sxs-lookup"><span data-stu-id="a5475-176">If the batch scoring process only happens a few times a day or less, this setting enables significant cost savings.</span></span>

<span data-ttu-id="a5475-177">Die automatische Skalierung ist ggf. nicht für Batchaufträge geeignet, die zeitlich zu nahe beieinander liegen.</span><span class="sxs-lookup"><span data-stu-id="a5475-177">Automatic scaling may not be appropriate for batch jobs that happen too close to each other.</span></span> <span data-ttu-id="a5475-178">Für die benötigte Zeit zum Erstellen und Entfernen eines Cluster fallen ebenfalls Kosten an. Das heißt, wenn eine Batchworkload nur wenige Minuten nach dem Ende des vorherigen Auftrags startet, ist es ggf. kostengünstiger den Cluster permanent, also auch zwischen den Aufträgen, auszuführen.</span><span class="sxs-lookup"><span data-stu-id="a5475-178">The time that it takes for a cluster to spin up and spin down also incur a cost, so if a batch workload begins only a few minutes after the previous job ends, it might be more cost effective to keep the cluster running between jobs.</span></span> <span data-ttu-id="a5475-179">Dies hängt davon ab, ob die Ausführung von Bewertungsprozessen mit hoher Häufigkeit (z.B. jede Stunde) oder geringerer Häufigkeit (z.B. einmal pro Monat) geplant ist.</span><span class="sxs-lookup"><span data-stu-id="a5475-179">That depends on whether scoring processes are scheduled to run at a high frequency (every hour, for example), or less frequently (once a month, for example).</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="a5475-180">Bereitstellen der Lösung</span><span class="sxs-lookup"><span data-stu-id="a5475-180">Deploy the solution</span></span>

<span data-ttu-id="a5475-181">Die Referenzimplementierung dieser Architektur ist auf [GitHub][github] verfügbar.</span><span class="sxs-lookup"><span data-stu-id="a5475-181">The reference implementation of this architecture is available on [GitHub][github].</span></span> <span data-ttu-id="a5475-182">Befolgen Sie die dort angegebenen Installationsschritte zum Erstellen einer skalierbaren Lösung für das parallele Bewerten mit Batch AI.</span><span class="sxs-lookup"><span data-stu-id="a5475-182">Follow the setup steps there to build a scalable solution for scoring many models in parallel using Batch AI.</span></span>

[acr]: /azure/container-registry/container-registry-intro
[ai]: /azure/application-insights/app-insights-overview
[automatic-scaling]: /azure/batch/batch-automatic-scaling
[azure-files]: /azure/storage/files/storage-files-introduction
[batch-ai]: /azure/batch-ai/
[bai-file-server]: /azure/batch-ai/resource-concepts#file-server
[create-resources]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/create_resources.ipynb
[deep]: /azure/architecture/reference-architectures/ai/batch-scoring-deep-learning
[event-hubs]: /azure/event-hubs/event-hubs-geo-dr
[explorer]: https://azure.microsoft.com/en-us/features/storage-explorer/
[github]: https://github.com/Azure/BatchAIAnomalyDetection
[logic-apps]: /azure/logic-apps/logic-apps-overview
[one-class-svm]: http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
[portal]: https://portal.azure.com
[python-script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/batchai/predict.py
[script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/sched/submit_jobs.py
[storage]: /azure/storage/blobs/storage-blobs-overview
[stream-analytics]: /azure/stream-analytics/
