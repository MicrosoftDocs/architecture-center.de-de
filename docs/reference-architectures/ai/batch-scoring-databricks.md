---
title: Batchbewertung von Spark-Modellen in Azure Databricks
description: Erstellen Sie eine skalierbare Lösung für die Batchbewertung eines Apache Spark-Klassifizierungsmodells nach einem Zeitplan mithilfe von Azure Databricks.
author: njray
ms.date: 02/07/2019
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai
ms.openlocfilehash: 1b6f10edf098ed8d9fa14c16de113fc765372835
ms.sourcegitcommit: a68f248402c598f9d25bc1dc62f27a6a934ff001
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 02/08/2019
ms.locfileid: "55903272"
---
# <a name="batch-scoring-of-spark-models-on-azure-databricks"></a><span data-ttu-id="c5239-103">Batchbewertung von Spark-Modellen in Azure Databricks</span><span class="sxs-lookup"><span data-stu-id="c5239-103">Batch scoring of Spark models on Azure Databricks</span></span>

<span data-ttu-id="c5239-104">Diese Referenzarchitektur veranschaulicht, wie mit Azure Databricks, einer für Azure optimierten Apache Spark-basierten Analyseplattform, eine skalierbare Lösung für die Batchbewertung eines Apache Spark-Klassifizierungsmodells nach einem Zeitplan erstellt wird.</span><span class="sxs-lookup"><span data-stu-id="c5239-104">This reference architecture shows how to build a scalable solution for batch scoring an Apache Spark classification model on a schedule using Azure Databricks, an Apache Spark-based analytics platform optimized for Azure.</span></span> <span data-ttu-id="c5239-105">Die Lösung kann als Vorlage verwendet werden, die für andere Szenarien generalisiert werden kann.</span><span class="sxs-lookup"><span data-stu-id="c5239-105">The solution can be used as a template that can be generalized to other scenarios.</span></span>

<span data-ttu-id="c5239-106">Eine Referenzimplementierung für diese Architektur ist auf [GitHub][github] verfügbar.</span><span class="sxs-lookup"><span data-stu-id="c5239-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Batchbewertung von Spark-Modellen in Azure Databricks](./_images/batch-scoring-spark.png)

<span data-ttu-id="c5239-108">**Szenario:** Ein Unternehmen in einer ressourcenlastigen Branche möchte die mit unerwarteten mechanischen Fehlern verbundenen Kosten und Ausfallzeiten minimieren.</span><span class="sxs-lookup"><span data-stu-id="c5239-108">**Scenario**: A business in an asset-heavy industry wants to minimize the costs and downtime associated with unexpected mechanical failures.</span></span> <span data-ttu-id="c5239-109">Mithilfe der von den Maschinen gesammelten IoT-Daten kann das Unternehmen ein Predictive Maintenance-Modell (Modell für die vorbeugende Wartung) erstellen.</span><span class="sxs-lookup"><span data-stu-id="c5239-109">Using IoT data collected from their machines, they can create a predictive maintenance model.</span></span> <span data-ttu-id="c5239-110">Dieses Modell ermöglicht es dem Unternehmen, Komponenten proaktiv zu warten und zu reparieren, bevor sie ausfallen.</span><span class="sxs-lookup"><span data-stu-id="c5239-110">This model enables the business to maintain components proactively and repair them before they fail.</span></span> <span data-ttu-id="c5239-111">Durch Maximieren der Verwendung der mechanischen Komponenten können Kosten kontrolliert und Ausfallzeiten reduziert werden.</span><span class="sxs-lookup"><span data-stu-id="c5239-111">By maximizing mechanical component use, they can control costs and reduce downtime.</span></span>

<span data-ttu-id="c5239-112">Ein Predictive Maintenance-Modell sammelt Daten von den Maschinen und speichert historische Beispiele von Komponentenausfällen.</span><span class="sxs-lookup"><span data-stu-id="c5239-112">A predictive maintenance model collects data from the machines and retains historical examples of component failures.</span></span> <span data-ttu-id="c5239-113">Das Modell kann dann verwendet werden, um den aktuellen Zustand der Komponenten zu überwachen und vorherzusagen, ob eine bestimmte Komponente in naher Zukunft ausfallen wird.</span><span class="sxs-lookup"><span data-stu-id="c5239-113">The model can then be used to monitor the current state of the components and predict if a given component will fail in the near future.</span></span> <span data-ttu-id="c5239-114">Allgemeine Anwendungsfälle und Modellierungsansätze finden Sie im [Azure AI-Leitfaden für Predictive Maintenance-Lösungen][ai-guide].</span><span class="sxs-lookup"><span data-stu-id="c5239-114">For common use cases and modeling approaches, see [Azure AI guide for predictive maintenance solutions][ai-guide].</span></span>

<span data-ttu-id="c5239-115">Diese Referenzarchitektur ist für Workloads konzipiert, die durch das Vorhandensein neuer Daten der Komponentenmaschinen ausgelöst werden.</span><span class="sxs-lookup"><span data-stu-id="c5239-115">This reference architecture is designed for workloads that are triggered by the presence of new data from the component machines.</span></span> <span data-ttu-id="c5239-116">Die Verarbeitung umfasst die folgenden Schritte:</span><span class="sxs-lookup"><span data-stu-id="c5239-116">Processing involves the following steps:</span></span>

1. <span data-ttu-id="c5239-117">Erfassen Sie die Daten aus dem externen Datenspeicher in einem Azure Databricks-Datenspeicher.</span><span class="sxs-lookup"><span data-stu-id="c5239-117">Ingest the data from the external data store onto an Azure Databricks data store.</span></span>

2. <span data-ttu-id="c5239-118">Trainieren Sie ein Machine Learning-Modell, indem Sie die Daten in ein Trainingsdataset transformieren, und erstellen Sie dann ein Spark MLlib-Modell.</span><span class="sxs-lookup"><span data-stu-id="c5239-118">Train a machine learning model by transforming the data into a training data set, then building a Spark MLlib model.</span></span> <span data-ttu-id="c5239-119">MLlib besteht aus den am häufigsten verwendeten Machine Learning-Algorithmen und -Dienstprogrammen, die für die Nutzung von Spark-Datenskalierbarkeitsfunktionen optimiert sind.</span><span class="sxs-lookup"><span data-stu-id="c5239-119">MLlib consists of most common machine learning algorithms and utilities optimized to take advantage of Spark data scalability capabilities.</span></span>

3. <span data-ttu-id="c5239-120">Wenden Sie das trainierte Modell an, um Komponentenausfälle vorherzusagen (zu klassifizieren), indem Sie die Daten in ein Bewertungsdataset transformieren.</span><span class="sxs-lookup"><span data-stu-id="c5239-120">Apply the trained model to predict (classify) component failures by transforming the data into a scoring data set.</span></span> <span data-ttu-id="c5239-121">Bewerten Sie die Daten mit dem Spark MLLib-Modell.</span><span class="sxs-lookup"><span data-stu-id="c5239-121">Score the data with the Spark MLLib model.</span></span>

4. <span data-ttu-id="c5239-122">Speichern Sie die Ergebnisse im Databricks-Datenspeicher zur Nachbearbeitung.</span><span class="sxs-lookup"><span data-stu-id="c5239-122">Store results on the Databricks data store for post-processing consumption.</span></span>

<span data-ttu-id="c5239-123">Auf  [GitHub][github] werden Notebooks für die Ausführung dieser Aufgaben bereitgestellt.</span><span class="sxs-lookup"><span data-stu-id="c5239-123">Notebooks are provided on [GitHub][github] to perform each of these tasks.</span></span>

## <a name="architecture"></a><span data-ttu-id="c5239-124">Architecture</span><span class="sxs-lookup"><span data-stu-id="c5239-124">Architecture</span></span>

<span data-ttu-id="c5239-125">Die Architektur definiert einen vollständig in [Azure Databricks][databricks] enthaltenen Datenfluss basierend auf einem Satz sequenziell ausgeführter [Notebooks][notebooks].</span><span class="sxs-lookup"><span data-stu-id="c5239-125">The architecture defines a data flow that is entirely contained within [Azure Databricks][databricks] based on a set of sequentially executed [notebooks][notebooks].</span></span> <span data-ttu-id="c5239-126">Die Architektur umfasst die folgenden Komponenten:</span><span class="sxs-lookup"><span data-stu-id="c5239-126">It consists of the following components:</span></span>

<span data-ttu-id="c5239-127">**[Datendateien][github]**:</span><span class="sxs-lookup"><span data-stu-id="c5239-127">**[Data files][github]**.</span></span> <span data-ttu-id="c5239-128">Bei der Referenzimplementierung wird ein simuliertes Dataset verwendet, das in fünf statischen Datendateien enthalten ist.</span><span class="sxs-lookup"><span data-stu-id="c5239-128">The reference implementation uses a simulated data set contained in five static data files.</span></span>

<span data-ttu-id="c5239-129">**[Erfassung][notebooks]**:</span><span class="sxs-lookup"><span data-stu-id="c5239-129">**[Ingestion][notebooks]**.</span></span> <span data-ttu-id="c5239-130">Das Datenerfassungsnotebook lädt die Eingabedatendateien in eine Sammlung von Databricks-Datasets herunter.</span><span class="sxs-lookup"><span data-stu-id="c5239-130">The data ingestion notebook downloads the input data files into a collection of Databricks data sets.</span></span> <span data-ttu-id="c5239-131">In einem realen Szenario würden Daten von IoT-Geräten in einen für Databricks zugänglichen Speicher wie Azure SQL Server oder Azure Blob Storage streamen.</span><span class="sxs-lookup"><span data-stu-id="c5239-131">In a real-world scenario, data from IoT devices would stream onto Databricks-accessible storage such as Azure SQL Server or Azure Blob storage.</span></span> <span data-ttu-id="c5239-132">Databricks unterstützt mehrere [Datenquellen][data-sources].</span><span class="sxs-lookup"><span data-stu-id="c5239-132">Databricks supports multiple [data sources][data-sources].</span></span>

<span data-ttu-id="c5239-133">**Trainingspipeline**:</span><span class="sxs-lookup"><span data-stu-id="c5239-133">**Training pipeline**.</span></span> <span data-ttu-id="c5239-134">Dieses Notebook führt das Featureentwicklungsnotebook aus, um ein Analysedataset aus den erfassten Daten zu erstellen.</span><span class="sxs-lookup"><span data-stu-id="c5239-134">This notebook executes the feature engineering notebook to create an analysis data set from the ingested data.</span></span> <span data-ttu-id="c5239-135">Dann führt es ein Modellerstellungsnotebook aus, das das Machine Learning-Modell mithilfe der skalierbaren Machine Learning-Bibliothek [Apache Spark MLlib][mllib] trainiert.</span><span class="sxs-lookup"><span data-stu-id="c5239-135">It then executes a model building notebook that trains the machine learning model using the [Apache Spark MLlib][mllib] scalable machine learning library.</span></span>

<span data-ttu-id="c5239-136">**Bewertungspipeline**:</span><span class="sxs-lookup"><span data-stu-id="c5239-136">**Scoring pipeline**.</span></span> <span data-ttu-id="c5239-137">Dieses Notebook führt das Featureentwicklungsnotebook aus, um ein Bewertungsdataset aus den erfassten Daten zu erstellen, und dann führt es das Bewertungsnotebook aus.</span><span class="sxs-lookup"><span data-stu-id="c5239-137">This notebook executes the feature engineering notebook to create scoring data set from the ingested data and executes the scoring notebook.</span></span> <span data-ttu-id="c5239-138">Das Bewertungsnotebook verwendet das trainierte [Spark MLlib][mllib-spark]-Modell, um Vorhersagen für die Beobachtungen im Bewertungsdataset zu generieren.</span><span class="sxs-lookup"><span data-stu-id="c5239-138">The scoring notebook uses the trained [Spark MLlib][mllib-spark] model to generate predictions for the observations in the scoring data set.</span></span> <span data-ttu-id="c5239-139">Die Vorhersagen werden im Ergebnisspeicher gespeichert. Dabei handelt es sich um ein neues Dataset für den Databricks-Datenspeicher.</span><span class="sxs-lookup"><span data-stu-id="c5239-139">The predictions are stored in the results store, a new data set on the Databricks data store.</span></span>

<span data-ttu-id="c5239-140">**Scheduler**:</span><span class="sxs-lookup"><span data-stu-id="c5239-140">**Scheduler**.</span></span> <span data-ttu-id="c5239-141">Ein geplanter Databricks-[Auftrag][job] verarbeitet die Batchbewertung mit dem Spark-Modell.</span><span class="sxs-lookup"><span data-stu-id="c5239-141">A scheduled Databricks [job][job] handles batch scoring with the Spark model.</span></span> <span data-ttu-id="c5239-142">Der Auftrag führt das Bewertungspipelinenotebook aus, wobei variable Argumente durch Notebookparameter übergeben werden, um die Details für das Erstellen des Bewertungsdatasets und den Speicherort für das Ergebnisdataset anzugeben.</span><span class="sxs-lookup"><span data-stu-id="c5239-142">The job executes the scoring pipeline notebook, passing variable arguments through notebook parameters to specify the details for constructing the scoring data set and where to store the results data set.</span></span>

<span data-ttu-id="c5239-143">Das Szenario ist als Pipelineflow konzipiert.</span><span class="sxs-lookup"><span data-stu-id="c5239-143">The scenario is constructed as a pipeline flow.</span></span> <span data-ttu-id="c5239-144">Jedes Notebook ist für die Ausführung in einer Batcheinstellung für jeden der folgenden Vorgänge optimiert: Erfassung, Featureentwicklung, Modellerstellung und Modellbewertung.</span><span class="sxs-lookup"><span data-stu-id="c5239-144">Each notebook is optimized to perform in a batch setting for each of the operations: ingestion, feature engineering, model building, and model scorings.</span></span> <span data-ttu-id="c5239-145">Zu diesem Zweck ist das Featureentwicklungsnotebook so konzipiert, dass es ein allgemeines Dataset für alle Trainings-, Kalibrierungs-, Test- oder Bewertungsvorgänge generiert.</span><span class="sxs-lookup"><span data-stu-id="c5239-145">To accomplish this, the feature engineering notebook is designed to generate a general data set for any of the training, calibration, testing, or scoring operations.</span></span> <span data-ttu-id="c5239-146">In diesem Szenario verwenden wir für diese Vorgänge eine temporale Aufteilungsstrategie, sodass die Notebookparameter zum Festlegen der Datumsbereichsfilterung verwendet werden.</span><span class="sxs-lookup"><span data-stu-id="c5239-146">In this scenario, we use a temporal split strategy for these operations, so the notebook parameters are used to set date-range filtering.</span></span>

<span data-ttu-id="c5239-147">Weil in dem Szenario eine Batchpipeline erstellt wird, stellen wir eine Reihe optionaler Überprüfungsnotebooks bereit, um die Ausgabe der Pipelinenotebooks zu untersuchen.</span><span class="sxs-lookup"><span data-stu-id="c5239-147">Because the scenario creates a batch pipeline, we provide a set of optional examination notebooks to explore the output of the pipeline notebooks.</span></span> <span data-ttu-id="c5239-148">Sie finden diese im GitHub-Repository:</span><span class="sxs-lookup"><span data-stu-id="c5239-148">You can find these in the GitHub repository:</span></span>

- `1a_raw-data_exploring`
- `2a_feature_exploration`
- `2b_model_testing`
- `3b_model_scoring_evaluation`

## <a name="recommendations"></a><span data-ttu-id="c5239-149">Empfehlungen</span><span class="sxs-lookup"><span data-stu-id="c5239-149">Recommendations</span></span>

<span data-ttu-id="c5239-150">Databricks ist so eingerichtet, dass Sie Ihre trainierten Modelle laden und bereitstellen können, um Vorhersagen mit neuen Daten zu treffen.</span><span class="sxs-lookup"><span data-stu-id="c5239-150">Databricks is set up so you can load and deploy your trained models to make predictions with new data.</span></span> <span data-ttu-id="c5239-151">Wir haben Databricks für dieses Szenario verwendet, weil diese Plattform die folgenden zusätzlichen Vorteile bietet:</span><span class="sxs-lookup"><span data-stu-id="c5239-151">We used Databricks for this scenario because it provides these additional advantages:</span></span>

- <span data-ttu-id="c5239-152">Unterstützung des einmaligen Anmeldens mit Azure Active Directory-Anmeldeinformationen</span><span class="sxs-lookup"><span data-stu-id="c5239-152">Single sign-on support using Azure Active Directory credentials.</span></span>
- <span data-ttu-id="c5239-153">Auftragsplaner zum Ausführen von Aufträgen für Produktionspipelines</span><span class="sxs-lookup"><span data-stu-id="c5239-153">Job scheduler to execute jobs for production pipelines.</span></span>
- <span data-ttu-id="c5239-154">Vollständig interaktives Notebook mit Zusammenarbeitsfunktionen, Dashboards und REST-APIs</span><span class="sxs-lookup"><span data-stu-id="c5239-154">Fully interactive notebook with collaboration, dashboards, REST APIs.</span></span>
- <span data-ttu-id="c5239-155">Unbegrenzte Cluster, die auf eine beliebige Größe skaliert werden können</span><span class="sxs-lookup"><span data-stu-id="c5239-155">Unlimited clusters that can scale to any size.</span></span>
- <span data-ttu-id="c5239-156">Erweiterte Sicherheit, rollenbasierte Zugriffssteuerungen und Überwachungsprotokolle</span><span class="sxs-lookup"><span data-stu-id="c5239-156">Advanced security, role-based access controls, and audit logs.</span></span>

<span data-ttu-id="c5239-157">Verwenden Sie für die Interaktion mit dem Azure Databricks-Dienst die Benutzeroberfläche des Databricks-[Arbeitsbereichs][workspace] in einem Webbrowser oder die Databricks-[Befehlszeilenschnittstelle][cli] (Command-Line Interface, CLI).</span><span class="sxs-lookup"><span data-stu-id="c5239-157">To interact with the Azure Databricks service, use the Databricks [Workspace][workspace] interface in a web browser or the [command-line interface][cli] (CLI).</span></span> <span data-ttu-id="c5239-158">Auf die Databricks-CLI können Sie über jede Plattform zugreifen, die Python 2.7.9 bis 3.6 unterstützt.</span><span class="sxs-lookup"><span data-stu-id="c5239-158">Access the Databricks CLI from any platform that supports Python 2.7.9 to 3.6.</span></span>

<span data-ttu-id="c5239-159">Bei der Referenzimplementierung werden [Notebooks][notebooks] verwendet, um Aufgaben nacheinander auszuführen.</span><span class="sxs-lookup"><span data-stu-id="c5239-159">The reference implementation uses [notebooks][notebooks] to execute tasks in sequence.</span></span> <span data-ttu-id="c5239-160">Jedes Notebook speichert Zwischendatenartefakte (Trainings-, Test-, Bewertungs- oder Ergebnisdatasets) im gleichen Datenspeicher wie die Eingabedaten.</span><span class="sxs-lookup"><span data-stu-id="c5239-160">Each notebook stores intermediate data artifacts (training, test, scoring, or results data sets) to the same data store as the input data.</span></span> <span data-ttu-id="c5239-161">Ziel ist es, Ihnen die Verwendung nach Bedarf in Ihrem bestimmten Anwendungsfall zu erleichtern.</span><span class="sxs-lookup"><span data-stu-id="c5239-161">The goal is to make it easy for you to use it as needed in your particular use case.</span></span> <span data-ttu-id="c5239-162">In der Praxis würden Sie Ihre Datenquelle mit Ihrer Azure Databricks-Instanz verbinden, damit die Notebooks direkt in Ihren Speicher lesen und schreiben können.</span><span class="sxs-lookup"><span data-stu-id="c5239-162">In practice, you would connect your data source to your Azure Databricks instance for the notebooks to read and write directly back into your storage.</span></span>

<span data-ttu-id="c5239-163">Sie können die Auftragsausführung je nach Bedarf über die Databricks-Benutzeroberfläche, den Datenspeicher oder die Databricks-[CLI][cli] überwachen.</span><span class="sxs-lookup"><span data-stu-id="c5239-163">You can monitor job execution through the Databricks user interface, the data store, or the Databricks [CLI][cli] as necessary.</span></span> <span data-ttu-id="c5239-164">Überwachen Sie den Cluster mithilfe des [Ereignisprotokolls][log] und anderer [Metriken][metrics], die Databricks bereitstellt.</span><span class="sxs-lookup"><span data-stu-id="c5239-164">Monitor the cluster using the [event log][log] and other [metrics][metrics] that Databricks provides.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="c5239-165">Überlegungen zur Leistung</span><span class="sxs-lookup"><span data-stu-id="c5239-165">Performance considerations</span></span>

<span data-ttu-id="c5239-166">Für einen Azure Databricks-Cluster ist die automatische Skalierung standardmäßig aktiviert, sodass Databricks während der Laufzeit Worker dynamisch neu zuordnen kann, um die Eigenschaften Ihres Auftrags zu berücksichtigen.</span><span class="sxs-lookup"><span data-stu-id="c5239-166">An Azure Databricks cluster enables autoscaling by default so that during runtime, Databricks dynamically reallocates workers to account for the characteristics of your job.</span></span> <span data-ttu-id="c5239-167">Bestimmte Teile der Pipeline sind möglicherweise anspruchsvoller und rechenintensiver als andere.</span><span class="sxs-lookup"><span data-stu-id="c5239-167">Certain parts of your pipeline may be more computationally demanding than others.</span></span> <span data-ttu-id="c5239-168">Databricks fügt während dieser Phasen Ihres Auftrags zusätzliche Worker hinzu (und entfernt diese, wenn sie nicht mehr benötigt werden).</span><span class="sxs-lookup"><span data-stu-id="c5239-168">Databricks adds additional workers during these phases of your job (and removes them when they’re no longer needed).</span></span> <span data-ttu-id="c5239-169">Durch die automatische Skalierung kann eine hohe [Clusterauslastung][cluster] einfacher erreicht werden, weil Sie den Cluster nicht entsprechend einer Workload bereitstellen müssen.</span><span class="sxs-lookup"><span data-stu-id="c5239-169">Autoscaling makes it easier to achieve high [cluster utilization][cluster], because you don’t need to provision the cluster to match a workload.</span></span>

<span data-ttu-id="c5239-170">Darüber hinaus können mithilfe von [Azure Data Factory][adf] mit Azure Databricks komplexere geplante Pipelines entwickelt werden.</span><span class="sxs-lookup"><span data-stu-id="c5239-170">Additionally, more complex scheduled pipelines can be developed by using [Azure Data Factory][adf] with Azure Databricks.</span></span>

## <a name="storage-considerations"></a><span data-ttu-id="c5239-171">Speicheraspekt</span><span class="sxs-lookup"><span data-stu-id="c5239-171">Storage considerations</span></span>

<span data-ttu-id="c5239-172">Bei dieser Referenzimplementierung werden die Daten der Einfachheit halber direkt im Databricks-Speicher gespeichert.</span><span class="sxs-lookup"><span data-stu-id="c5239-172">In this reference implementation, the data is stored directly within Databricks storage for simplicity.</span></span> <span data-ttu-id="c5239-173">In einer Produktionsumgebung können die Daten jedoch in einem Clouddatenspeicher wie [Azure Blob Storage][blob] gespeichert werden.</span><span class="sxs-lookup"><span data-stu-id="c5239-173">In a production setting, however, the data can be stored on cloud data storage such as [Azure Blob Storage][blob].</span></span> <span data-ttu-id="c5239-174">[Databricks][databricks-connect] unterstützt auch Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka und Hadoop.</span><span class="sxs-lookup"><span data-stu-id="c5239-174">[Databricks][databricks-connect] also supports Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka, and Hadoop.</span></span>

## <a name="cost-considerations"></a><span data-ttu-id="c5239-175">Kostenbetrachtung</span><span class="sxs-lookup"><span data-stu-id="c5239-175">Cost considerations</span></span>

<span data-ttu-id="c5239-176">Azure Databricks ist ein Spark-Premiumangebot, das mit Kosten verbunden ist.</span><span class="sxs-lookup"><span data-stu-id="c5239-176">Azure Databricks is a premium Spark offering with an associated cost.</span></span> <span data-ttu-id="c5239-177">Darüber hinaus gibt es [Tarife][pricing] für Databricks Standard und Databricks Premium.</span><span class="sxs-lookup"><span data-stu-id="c5239-177">In addition, there are standard and premium Databricks [pricing tiers][pricing].</span></span>

<span data-ttu-id="c5239-178">In diesem Szenario ist der Tarif „Standard“ ausreichend.</span><span class="sxs-lookup"><span data-stu-id="c5239-178">For this scenario, the standard pricing tier is sufficient.</span></span> <span data-ttu-id="c5239-179">Wenn Ihre spezielle Anwendung jedoch eine automatische Skalierung von Clustern zur Bewältigung größerer Workloads oder für interaktive Databricks-Dashboards erfordert, könnten sich die Kosten durch den Tarif „Premium“ weiter erhöhen.</span><span class="sxs-lookup"><span data-stu-id="c5239-179">However, if your specific application requires automatically scaling clusters to handle larger workloads or interactive Databricks dashboards, the premium level could increase costs further.</span></span>

<span data-ttu-id="c5239-180">Die Lösungsnotebooks können auf jeder Spark-basierten Plattform mit minimalen Änderungen zum Entfernen der Databricks-spezifischen Pakete ausgeführt werden.</span><span class="sxs-lookup"><span data-stu-id="c5239-180">The solution notebooks can run on any Spark-based platform with minimal edits to remove the Databricks-specific packages.</span></span> <span data-ttu-id="c5239-181">Unter den folgenden Links finden Sie ähnliche Lösungen für verschiedene Azure-Plattformen:</span><span class="sxs-lookup"><span data-stu-id="c5239-181">See the following similar solutions for various Azure platforms:</span></span>

- <span data-ttu-id="c5239-182">[Python für Azure Machine Learning Studio][python-aml]</span><span class="sxs-lookup"><span data-stu-id="c5239-182">[Python on Azure Machine Learning Studio][python-aml]</span></span>
- <span data-ttu-id="c5239-183">[SQL Server R Services][sql-r]</span><span class="sxs-lookup"><span data-stu-id="c5239-183">[SQL Server R services][sql-r]</span></span>
- <span data-ttu-id="c5239-184">[PySpark für Azure Data Science Virtual Machine][py-dvsm]</span><span class="sxs-lookup"><span data-stu-id="c5239-184">[PySpark on an Azure Data Science Virtual Machine][py-dvsm]</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="c5239-185">Bereitstellen der Lösung</span><span class="sxs-lookup"><span data-stu-id="c5239-185">Deploy the solution</span></span>

<span data-ttu-id="c5239-186">Führen Sie zum Bereitstellen dieser Referenzarchitektur die im [GitHub][github]-Repository beschriebenen Schritte aus, um eine skalierbare Lösung für die Batchbewertung von Spark-Modellen in Azure Databricks zu erstellen.</span><span class="sxs-lookup"><span data-stu-id="c5239-186">To deploy this reference architecture, follow the steps described in the [GitHub][github] repository to build a scalable solution for scoring Spark models in batch on Azure Databricks.</span></span>

## <a name="related-architectures"></a><span data-ttu-id="c5239-187">Verwandte Architekturen</span><span class="sxs-lookup"><span data-stu-id="c5239-187">Related architectures</span></span>

<span data-ttu-id="c5239-188">Wir haben auch eine Referenzarchitektur erstellt, die Spark zum Erstellen von [Echtzeitempfehlungssystemen][recommendation] mit vorab berechneten Offlinebewertungen verwendet.</span><span class="sxs-lookup"><span data-stu-id="c5239-188">We have also built a reference architecture that uses Spark for building [real-time recommendation systems][recommendation] with offline, pre-computed scores.</span></span> <span data-ttu-id="c5239-189">Diese Empfehlungssysteme werden häufig in Szenarien verwendet, in denen eine Batchverarbeitung von Bewertungen erfolgt.</span><span class="sxs-lookup"><span data-stu-id="c5239-189">These recommendation systems are common scenarios where scores are batch-processed.</span></span>

[adf]: https://azure.microsoft.com/blog/operationalize-azure-databricks-notebooks-using-data-factory/
[ai-guide]: /azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance
[blob]: https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[cli]: https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html
[cluster]: https://docs.azuredatabricks.net/user-guide/clusters/sizing.html
[databricks]: /azure/azure-databricks/
[databricks-connect]: /azure/azure-databricks/databricks-connect-to-data-sources
[data-sources]: https://docs.databricks.com/spark/latest/data-sources/index.html
[github]: https://github.com/Azure/BatchSparkScoringPredictiveMaintenance
[job]: https://docs.databricks.com/user-guide/jobs.html
[log]: https://docs.databricks.com/user-guide/clusters/event-log.html
[metrics]: https://docs.databricks.com/user-guide/clusters/metrics.html
[mllib]: https://docs.databricks.com/spark/latest/mllib/index.html
[mllib-spark]: https://docs.databricks.com/spark/latest/mllib/index.html#apache-spark-mllib
[notebooks]: https://docs.databricks.com/user-guide/notebooks/index.html
[pricing]: https://azure.microsoft.com/en-us/pricing/details/databricks/
[python-aml]: https://gallery.azure.ai/Notebook/Predictive-Maintenance-Modelling-Guide-Python-Notebook-1
[py-dvsm]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-using-PySpark
[recommendation]: /azure/architecture/reference-architectures/ai/real-time-recommendation
[sql-r]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-Modeling-Guide-using-SQL-R-Services-1
[workspace]: https://docs.databricks.com/user-guide/workspace.html
